{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Scaling Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do custom scaling due to the following limitations:\n",
    "\n",
    "1. MinMaxScaler in MLlib does not work for us, because it works on Vector objects\n",
    "2. MinMaxScaler in Scikit Learn works on Pandas DataFrames, but Pandas DataFrames do not scale\n",
    "3. Using UDF in Spark SQL is too cumbersome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data and convert to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.types import StructType, StructField, LongType\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "app_name = \"scaling_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!head -n 10 data/train_100K.txt > data/train_10.txt\n",
    "#!head -n 20 data/train_100K.txt > data/test_10.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallRDD = sc.textFile('data/train_10.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertNumber(idx, num):\n",
    "    if num != '':\n",
    "        if idx > 13:\n",
    "            return int(num, 16)\n",
    "        else:\n",
    "            return int(num)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallRDD2 = smallRDD.map(lambda x: [ConvertNumber(idx, num) for idx,num in enumerate(x.split('\\t'))]).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallRDDRow = smallRDD2.take(1)\n",
    "numColumns = len(smallRDDRow[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "structFieldList = [StructField('field_' + str(num), LongType(), True) for num in range(numColumns)]\n",
    "schema = StructType(structFieldList)\n",
    "testDF = spark.createDataFrame(smallRDD2, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the fields of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+\n",
      "|field_0|field_2|field_5|\n",
      "+-------+-------+-------+\n",
      "|      0|      1|   1382|\n",
      "|      0|      0|    102|\n",
      "|      0|      0|    767|\n",
      "|      0|    893|   4392|\n",
      "|      0|     -1|      2|\n",
      "|      0|     -1|  12824|\n",
      "|      0|      1|   3168|\n",
      "|      1|      4|      0|\n",
      "|      0|     44|  19010|\n",
      "|      0|     35|  33737|\n",
      "+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDF.select(['field_0', 'field_2', 'field_5']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define scaling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleRow(row):\n",
    "    rowDict = row.asDict()\n",
    "    \n",
    "    # Scale by subtracting the min, and dividing by the delta\n",
    "    for field in scaleDict.keys():\n",
    "        temp = float(rowDict[field]-scaleDict[field][0])/scaleDict[field][1]\n",
    "        \n",
    "        # The test set may have data that is outside the range in the training set.\n",
    "        # Limit the range to 0-1.\n",
    "        rowDict[field] = min(max(temp, 0.0), 1.0)\n",
    "        \n",
    "    return pyspark.sql.Row(**rowDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleDataFrame_fit(fields, df):\n",
    "\n",
    "    # Note: Need to rename the 'summary' column, because using it in the filter statement tries to invoke the function\n",
    "    summaryDF = df.select(fields).summary(['min', 'max']).withColumnRenamed('summary', 'summary_col').cache()\n",
    "    \n",
    "    minRow = summaryDF.filter(summaryDF.summary_col == 'min').first()\n",
    "    maxRow = summaryDF.filter(summaryDF.summary_col == 'max').first()\n",
    "    \n",
    "    for field in fields:\n",
    "        min = int(minRow[field])\n",
    "        max = int(maxRow[field])    \n",
    "        scaleDict[field] = (min, max-min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleDataFrame_transform(df):\n",
    "    return df.rdd.map(scaleRow).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define an empty scaling dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaleDict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the selected fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaleDataFrame_fit(['field_2', 'field_5'], testDF)\n",
    "testDF2 = scaleDataFrame_transform(testDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|field_0|             field_2|             field_5|\n",
      "+-------+--------------------+--------------------+\n",
      "|      0|0.002237136465324...|  0.0409639268458962|\n",
      "|      0|0.001118568232662...|0.003023386786021...|\n",
      "|      0|0.001118568232662...| 0.02273468298900317|\n",
      "|      0|                 1.0|  0.1301834780804458|\n",
      "|      0|                 0.0|5.928209384355455...|\n",
      "|      0|                 0.0|  0.3801167857248718|\n",
      "|      0|0.002237136465324...| 0.09390283664819041|\n",
      "|      1|0.005592841163310962|                 0.0|\n",
      "|      0|0.050335570469798654|   0.563476301982986|\n",
      "|      0|0.040268456375838924|                 1.0|\n",
      "+-------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDF2.select(['field_0', 'field_2', 'field_5']).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
