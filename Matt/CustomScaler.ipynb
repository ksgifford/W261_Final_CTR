{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Scaling Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do custom scaling due to the following limitations:\n",
    "\n",
    "1. MinMaxScaler in MLlib does not work for us, because it works on Vector objects\n",
    "2. MinMaxScaler in Scikit Learn works on Pandas DataFrames, but Pandas DataFrames do not scale\n",
    "3. Using UDF in Spark SQL is too cumbersome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data and convert to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.types import StructType, StructField, LongType\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "app_name = \"scaling_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!head -n 10 data/train_100K.txt > data/train_10.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallRDD = sc.textFile('data/train_10.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertNumber(idx, num):\n",
    "    if num != '':\n",
    "        if idx > 13:\n",
    "            return int(num, 16)\n",
    "        else:\n",
    "            return int(num)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallRDD2 = smallRDD.map(lambda x: [ConvertNumber(idx, num) for idx,num in enumerate(x.split('\\t'))]).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallRDDRow = smallRDD2.take(1)\n",
    "numColumns = len(smallRDDRow[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "structFieldList = [StructField('field_' + str(num), LongType(), True) for num in range(numColumns)]\n",
    "schema = StructType(structFieldList)\n",
    "testDF = spark.createDataFrame(smallRDD2, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the fields of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+\n",
      "|field_0|field_2|field_5|\n",
      "+-------+-------+-------+\n",
      "|      0|      1|   1382|\n",
      "|      0|      0|    102|\n",
      "|      0|      0|    767|\n",
      "|      0|    893|   4392|\n",
      "|      0|     -1|      2|\n",
      "|      0|     -1|  12824|\n",
      "|      0|      1|   3168|\n",
      "|      1|      4|      0|\n",
      "|      0|     44|  19010|\n",
      "|      0|     35|  33737|\n",
      "+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDF.select(['field_0', 'field_2', 'field_5']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define scaling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleRow(row):\n",
    "    rowDict = row.asDict()\n",
    "    \n",
    "    # Scale by subtracting the min, and dividing by the delta\n",
    "    for field in scaleDict.keys():\n",
    "        rowDict[field] = (rowDict[field]-scaleDict[field][0])/scaleDict[field][1]\n",
    "        \n",
    "    return pyspark.sql.Row(**rowDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleDataFrame(fields, df):\n",
    "\n",
    "    # Note: Need to rename the 'summary' column, because using it in the filter statement tries to invoke the function\n",
    "    summaryDF = df.select(fields).summary(['min', 'max']).withColumnRenamed('summary', 'summary_col').cache()\n",
    "    \n",
    "    minRow = summaryDF.filter(summaryDF.summary_col == 'min').first()\n",
    "    maxRow = summaryDF.filter(summaryDF.summary_col == 'max').first()\n",
    "    \n",
    "    for field in fields:\n",
    "        min = int(minRow[field])\n",
    "        max = int(maxRow[field])    \n",
    "        scaleDict[field] = (min, max-min)\n",
    "        \n",
    "    return df.rdd.map(scaleRow).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define an empty scaling dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaleDict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the selected fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF2 = scaleDataFrame(['field_2', 'field_5'], testDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|field_0|             field_2|             field_5|\n",
      "+-------+--------------------+--------------------+\n",
      "|      0|0.002237136465324...|  0.0409639268458962|\n",
      "|      0|0.001118568232662...|0.003023386786021...|\n",
      "|      0|0.001118568232662...| 0.02273468298900317|\n",
      "|      0|                 1.0|  0.1301834780804458|\n",
      "|      0|                 0.0|5.928209384355455...|\n",
      "|      0|                 0.0|  0.3801167857248718|\n",
      "|      0|0.002237136465324...| 0.09390283664819041|\n",
      "|      1|0.005592841163310962|                 0.0|\n",
      "|      0|0.050335570469798654|   0.563476301982986|\n",
      "|      0|0.040268456375838924|                 1.0|\n",
      "+-------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDF2.select(['field_0', 'field_2', 'field_5']).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
