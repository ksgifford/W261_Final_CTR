{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Test #3 (Using Scikit Learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, LongType\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql\n",
    "import pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "app_name = \"data_cleaning_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do this on the command line first: hadoop fs -mkdir /RddCheckPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.setCheckpointDir(\"hdfs://quickstart.cloudera:8020/RddCheckPoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainRDD = sc.textFile('data/train1percentsample.txt')\n",
    "#testRDD = sc.textFile('data/test1percentsample.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertNumber(idx, num):\n",
    "    if num != '':\n",
    "        if idx > 13:\n",
    "            return int(num, 16)\n",
    "        else:\n",
    "            return int(num)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainRDD2 = trainRDD.map(lambda x: [ConvertNumber(idx, num) for idx,num in enumerate(x.split('\\t'))]).cache()\n",
    "#testRDD2 = testRDD.map(lambda x: [ConvertNumber(idx, num) for idx,num in enumerate(x.split('\\t'))]).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainRDDRow = trainRDD2.take(1)\n",
    "numColumns = len(trainRDDRow[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "structFieldList = [StructField('field_' + str(num), LongType(), True) for num in range(numColumns)]\n",
    "schema = StructType(structFieldList)\n",
    "\n",
    "trainDF = spark.createDataFrame(trainRDD2, schema)\n",
    "#testDF = spark.createDataFrame(testRDD2, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constrainRow(row):\n",
    "    rowDict = row.asDict()\n",
    "    \n",
    "    for field in valueDict.keys():\n",
    "\n",
    "        if rowDict[field] not in valueDict[field]:\n",
    "            # Set to unknown / rare\n",
    "            rowDict[field] = 1000    # sys.maxint\n",
    "        else:\n",
    "            # Set to the new index. This is so existing values do not collide with the unknown / rare value (1000)\n",
    "            rowDict[field] = valueDict[field][rowDict[field]]\n",
    "    \n",
    "    return pyspark.sql.Row(**rowDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oheDataFrame_getCategories(fieldList, df):\n",
    "\n",
    "    for field in fieldList:\n",
    "        # Find the frequency of items in the category\n",
    "        fieldFreqRDD = df.rdd.map(lambda x: (x[field[0]], 1)).\\\n",
    "                              reduceByKey(lambda x, y: x+y)\n",
    "\n",
    "        validValuesDict = {}\n",
    "        index=0\n",
    "        for value in fieldFreqRDD.takeOrdered(field[1], key=lambda x: -x[1]):\n",
    "            validValuesDict[value[0]] = index\n",
    "            index += 1\n",
    "\n",
    "        # Use the top N frequent values\n",
    "        valueDict[field[0]] = validValuesDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oheDataFrame_transform(df):\n",
    "    oheRDD = df.rdd.map(lambda row: constrainRow(row))\n",
    "    return oheRDD.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Optimize this to do all columns at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputeWithMean(field, df):\n",
    "    fieldMean = df.rdd.map(lambda row: row[field]).filter(lambda x: x != None).mean()\n",
    "    return df.fillna(fieldMean, [field])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started processing the fields...\n"
     ]
    }
   ],
   "source": [
    "print('Started processing the fields...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has about 42% null values.  Drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_1')\n",
    "#testDF = testDF.drop('field_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No null values, so scale this field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has about 23% null values.  Drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_3')\n",
    "#testDF = testDF.drop('field_3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has about 25% null values.  Drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_4')\n",
    "#testDF = testDF.drop('field_4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has about 3% null values.  Impute with the mean and scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = imputeWithMean('field_5', trainDF)\n",
    "#testDF = imputeWithMean('field_5', testDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has about 22% null values. Drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_6')\n",
    "#testDF = testDF.drop('field_6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has about 4% null values. Impute with the mean and scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = imputeWithMean('field_7', trainDF)\n",
    "#testDF = imputeWithMean('field_7', testDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has < 1% null values. Impute with the mean and scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = imputeWithMean('field_8', trainDF)\n",
    "#testDF = imputeWithMean('field_8', testDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 4% null values. Impute with the mean and scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = imputeWithMean('field_9', trainDF)\n",
    "#testDF = imputeWithMean('field_9', testDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 42% null values. Drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_10')\n",
    "#testDF = testDF.drop('field_10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 4% null values. Impute with the mean and scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = imputeWithMean('field_11', trainDF)\n",
    "#testDF = imputeWithMean('field_11', testDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 77% null values. Drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_12')\n",
    "#testDF = testDF.drop('field_12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 25% null values. Drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_13')\n",
    "#testDF = testDF.drop('field_13')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 0% null values. It has a modest amount of distinct values, so OHE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 0% null values. It has a modest amount of distinct values, so OHE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 4% null values. It has a large amount of distinct values, so drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_16')\n",
    "#testDF = testDF.drop('field_16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 4% null values. It has a large amount of distinct values, so drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_17')\n",
    "#testDF = testDF.drop('field_17')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 0% null values. It has a modest amount of distinct values, so OHE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 12% null values, so delete those observations (NOTE: this is categorical, so we can't impute).\n",
    "It has a modest amount of distinct values, so OHE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 0% null values. It has a modest amount of distinct values, so OHE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 0% null values. It has a modest amount of distinct values, so OHE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 0% null values. It has a small amount of distinct values, so OHE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 0% null values. It has a large amount of distinct values, so drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_23')\n",
    "#testDF = testDF.drop('field_23')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 0% null values. It has a modest amount of distinct values, so OHE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 4% null values. It has a large amount of distinct values, so drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_25')\n",
    "#testDF = testDF.drop('field_25')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 0% null values. It has a modest amount of distinct values, so OHE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 0% null values. It has a small amount of distinct values, so OHE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 0% null values. It has a modest amount of distinct values, so OHE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 4% null values. It has a large amount of distinct values, so drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_29')\n",
    "#testDF = testDF.drop('field_29')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 0% null values. It has a small amount of distinct values, so OHE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 0% null values. It has a modest amount of distinct values, so OHE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 48% null values, so drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_32')\n",
    "#testDF = testDF.drop('field_32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 48% null values, so drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_33')\n",
    "#testDF = testDF.drop('field_33')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 4% null values. It has a large amount of distinct values, so drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_34')\n",
    "#testDF = testDF.drop('field_34')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 74% null values, so drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_35')\n",
    "#testDF = testDF.drop('field_35')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 0% null values. It has a small amount of distinct values, so OHE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 4% null values. It has a large amount of distinct values, so drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_37')\n",
    "#testDF = testDF.drop('field_37')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 48% null values, so drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_38')\n",
    "#testDF = testDF.drop('field_38')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 48% null values, so drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_39')\n",
    "#testDF = testDF.drop('field_39')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Finished processing the fields\n"
     ]
    }
   ],
   "source": [
    "print('...Finished processing the fields')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete null observations in a batch (this is for categorical values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.dropna(how='any', subset=['field_19'])\n",
    "#testDF = testDF.dropna(how='any', subset=['field_19'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Do Imputation In a Batch (this is for numerical values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started OHE...\n"
     ]
    }
   ],
   "source": [
    "print('Started OHE...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess columns to renumber values so that OHE will create the appropriate number of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "valueDict = {}\n",
    "oheFieldsAndSizes = [('field_14',10),('field_15',10),('field_18',10),('field_19',10),('field_20',10),('field_21',10),('field_22',3),('field_24',10),('field_26',10),('field_27',10),('field_28',10),('field_30',10),('field_31',10),('field_36',10)]\n",
    "oheFields = [x[0] for x in oheFieldsAndSizes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "oheDataFrame_getCategories(oheFieldsAndSizes, trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = oheDataFrame_transform(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainDF.show(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Scikit Learn to OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a Pandas DF\n",
    "trainDF_pd = trainDF.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainDF_pd.iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(trainDF_pd[oheFields])\n",
    "trainDF_ohe_sparse = enc.transform(trainDF_pd[oheFields])\n",
    "trainDF_ohe_dense = trainDF_ohe_sparse.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Finished OHE\n"
     ]
    }
   ],
   "source": [
    "print('...Finished OHE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Scikit Learn to Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started scaling...\n"
     ]
    }
   ],
   "source": [
    "print('Started scaling...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericFields = ['field_0','field_2','field_5','field_7','field_8','field_9','field_11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(trainDF_pd[numericFields])\n",
    "trainDF_scaled = scaler.transform(trainDF_pd[numericFields])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Finished scaling\n"
     ]
    }
   ],
   "source": [
    "print('...Finished scaling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recombine the matricies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF_combined = np.hstack((trainDF_scaled, trainDF_ohe_dense))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reclaim memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del trainDF_scaled\n",
    "del trainDF_ohe_dense\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert back to a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started converting to DataFrame...\n"
     ]
    }
   ],
   "source": [
    "print('Started converting to DataFrame...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicies = [i for i in range(trainDF_combined.shape[0])]\n",
    "columnNames = [str(x) for x in range(trainDF_combined.shape[1])]\n",
    "trainDF = spark.createDataFrame(pd.DataFrame(data=trainDF_combined, index=indicies, columns=columnNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Finished converting to DataFrame\n"
     ]
    }
   ],
   "source": [
    "print('...Finished converting to DataFrame')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reclaim memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del trainDF_combined\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save cleaned DataFrame to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started saving the DataFrames...\n"
     ]
    }
   ],
   "source": [
    "print('Started saving the DataFrames...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-00093116343f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data/train1percentsample_cleaned_tmp.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#testDF.write.csv(path='data/test1percentsample_cleaned_tmp.txt', header=False, sep='\\t')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping)\u001b[0m\n\u001b[1;32m    883\u001b[0m                        \u001b[0mignoreTrailingWhiteSpace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignoreTrailingWhiteSpace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m                        charToEscapeQuoteEscaping=charToEscapeQuoteEscaping)\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/py4j-0.10.7-py3.6.egg/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/py4j-0.10.7-py3.6.egg/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/py4j-0.10.7-py3.6.egg/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainDF.write.csv(path='data/train1percentsample_cleaned_tmp.txt', header=False, sep='\\t')\n",
    "#testDF.write.csv(path='data/test1percentsample_cleaned_tmp.txt', header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf data/train1percentsample_cleaned.txt\n",
    "#!rm -rf data/test1percentsample_cleaned.txt\n",
    "!mv data/train1percentsample_cleaned_tmp.txt data/train1percentsample_cleaned.txt\n",
    "#!mv data/test1percentsample_cleaned_tmp.txt data/test1percentsample_cleaned.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('...Finished saving the DataFrames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The total time was: {} seconds'.format(time.time() - startTime))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
