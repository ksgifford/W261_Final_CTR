{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Handling Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data and convert to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.types import StructType, StructField, LongType\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql\n",
    "import pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "app_name = \"categorical_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallRDD2 = sc.parallelize([(1,1), (2,1), (2,1), (3,1), (3,1), (3,1), (4,1), (4,1), (4,1), (4,1)])\n",
    "numColumns = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "structFieldList = [StructField('field_' + str(num), LongType(), True) for num in range(numColumns)]\n",
    "schema = StructType(structFieldList)\n",
    "testDF = spark.createDataFrame(smallRDD2, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the fields of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|field_0|\n",
      "+-------+\n",
      "|      1|\n",
      "|      2|\n",
      "|      2|\n",
      "|      3|\n",
      "|      3|\n",
      "|      3|\n",
      "|      4|\n",
      "|      4|\n",
      "|      4|\n",
      "|      4|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDF.select(['field_0']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once all null observations have been removed, add a primary key index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF2 = testDF.withColumn('id', pyspark.sql.functions.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------+\n",
      "|field_0|field_1|         id|\n",
      "+-------+-------+-----------+\n",
      "|      1|      1|          0|\n",
      "|      2|      1|          1|\n",
      "|      2|      1| 8589934592|\n",
      "|      3|      1| 8589934593|\n",
      "|      3|      1|17179869184|\n",
      "|      3|      1|17179869185|\n",
      "|      4|      1|25769803776|\n",
      "|      4|      1|25769803777|\n",
      "|      4|      1|25769803778|\n",
      "|      4|      1|25769803779|\n",
      "+-------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDF2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createOneHotEncodingForRow(value):\n",
    "\n",
    "    # Encode the rare / unseen value\n",
    "    if value not in valueDict:\n",
    "        oheList = [0] * len(valueDict)\n",
    "        oheList.append(1)\n",
    "        return oheList\n",
    "\n",
    "    oheList = []\n",
    "\n",
    "    # Encode values that have been seen\n",
    "    for key in valueDict.keys():\n",
    "        if key == value:\n",
    "            oheList.append(1)\n",
    "        else:\n",
    "            oheList.append(0)\n",
    "    \n",
    "    # Last column is for rare / unseen. Set to 0.\n",
    "    oheList.append(0)\n",
    "    \n",
    "    return oheList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createOHEDataFrame(field, topN, df):\n",
    "    \n",
    "    # Find the frequency of items in the category\n",
    "    fieldFreqRDD = df.select(field).rdd.map(lambda x: (x[field], 1)).\\\n",
    "                                        reduceByKey(lambda x, y: x+y).\\\n",
    "                                        sortBy(lambda x: -x[1])\n",
    "\n",
    "    # Use the top N frequent values\n",
    "    valueCountList = fieldFreqRDD.take(topN)\n",
    "\n",
    "    # Add those values to a dictionary for OHE\n",
    "    for pair in valueCountList:\n",
    "        valueDict[pair[0]] = True\n",
    "\n",
    "    # Create a new RDD with the encoded values\n",
    "    oheRDD = df.rdd.map(lambda row: createOneHotEncodingForRow(row[field]))\n",
    "\n",
    "    # Create a new DataFrame for the OHE RDD\n",
    "    structFieldList = [StructField(field + '_' + str(num), LongType(), True) for num in range(len(valueDict))]\n",
    "    structFieldList.append(StructField(field + '_UnkwnRare', LongType(), True))\n",
    "    schema = StructType(structFieldList)\n",
    "    oheDF = spark.createDataFrame(oheRDD, schema)\n",
    "\n",
    "    # Add an index column\n",
    "    oheDF = oheDF.withColumn('id', pyspark.sql.functions.monotonically_increasing_id())\n",
    "\n",
    "    # Join the original DataFrame with the OHE DataFrame\n",
    "    #updatedDF = df.join(oheDF, df.id == oheDF.id, 'inner').drop(oheDF.id)\n",
    "    updatedDF = df.join(oheDF, [\"id\"])\n",
    "    \n",
    "    # Drop the original field that was OHE\n",
    "    updatedDF = updatedDF.drop(field)\n",
    "    \n",
    "    return updatedDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary for the values to create new fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "valueDict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the function to OHE the field to the number of specified fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF3 = createOHEDataFrame('field_0', 3, testDF2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the DataFrame with the OHE fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+---------+---------+---------+-----------------+\n",
      "|         id|field_1|field_0_0|field_0_1|field_0_2|field_0_UnkwnRare|\n",
      "+-----------+-------+---------+---------+---------+-----------------+\n",
      "|          0|      1|        0|        0|        0|                1|\n",
      "|          1|      1|        0|        0|        1|                0|\n",
      "| 8589934592|      1|        0|        0|        1|                0|\n",
      "| 8589934593|      1|        0|        1|        0|                0|\n",
      "|17179869184|      1|        0|        1|        0|                0|\n",
      "|17179869185|      1|        0|        1|        0|                0|\n",
      "|25769803776|      1|        1|        0|        0|                0|\n",
      "|25769803777|      1|        1|        0|        0|                0|\n",
      "|25769803778|      1|        1|        0|        0|                0|\n",
      "|25769803779|      1|        1|        0|        0|                0|\n",
      "+-----------+-------+---------+---------+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDF3.orderBy('id').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
