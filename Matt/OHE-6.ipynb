{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Handling Example (to support OHE and feature hashing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data and convert to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql\n",
    "import pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "app_name = \"categorical_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallRDD2 = sc.parallelize([(1,1,'a',3,121), (2,2,'b',3,230), (2,3,'c',1,1056), (3,4,'a',2,4587), (3,5,'b',2,9546), (3,6,'c',4,1218), (4,7,'a',4,463), (4,8,'a',4,571), (4,9,'c',4,32), (4,10,'c',4,799)])\n",
    "numColumns = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "structFieldList = [StructField('field_0', LongType(), True),StructField('field_1', LongType(), True),StructField('field_2', StringType(), True),StructField('field_11', StringType(), True),StructField('field_12', LongType(), True)]\n",
    "schema = StructType(structFieldList)\n",
    "testDF = spark.createDataFrame(smallRDD2, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the fields of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+--------+--------+\n",
      "|field_0|field_1|field_2|field_11|field_12|\n",
      "+-------+-------+-------+--------+--------+\n",
      "|      1|      1|      a|       3|     121|\n",
      "|      2|      2|      b|       3|     230|\n",
      "|      2|      3|      c|       1|    1056|\n",
      "|      3|      4|      a|       2|    4587|\n",
      "|      3|      5|      b|       2|    9546|\n",
      "|      3|      6|      c|       4|    1218|\n",
      "|      4|      7|      a|       4|     463|\n",
      "|      4|      8|      a|       4|     571|\n",
      "|      4|      9|      c|       4|      32|\n",
      "|      4|     10|      c|       4|     799|\n",
      "+-------+-------+-------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertRowToArray(row):\n",
    "    rowDict = row.asDict()\n",
    "    \n",
    "    X = np.array([])\n",
    "    \n",
    "    # Iterate over fields in the row\n",
    "    for field in rowDict.keys():\n",
    "        \n",
    "        # If the field is categorical and to be OHE\n",
    "        if field in valueDict:\n",
    "            \n",
    "            if rowDict[field] not in valueDict[field]:\n",
    "                \n",
    "                # If the value is not found in the categories for that field (rare/unknown),\n",
    "                # then the encoding is all zeros\n",
    "                \n",
    "                X = np.append(X, np.zeros(cardinalityDict[field]))\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # If the value is found in the categories for that field      \n",
    "                ohe = np.zeros(cardinalityDict[field])\n",
    "                \n",
    "                # Look up the value in the dictionary for this category (it is an index)\n",
    "                index = valueDict[field][rowDict[field]]\n",
    "                ohe[index] = 1\n",
    "                X = np.append(X, ohe)\n",
    "                \n",
    "        # If the field is categorical and to be feature hashed\n",
    "        elif field in featureHashingDict:\n",
    "            \n",
    "            # Get the cardinality and create a zeros array. Note, to encode 3 values, use 3-1 bits, e.g. (00, 10, 01)\n",
    "            cardinality = featureHashingDict[field]\n",
    "            hashEnc = np.zeros(cardinality-1)\n",
    "            \n",
    "            # Encode the field % cardinality\n",
    "            index = rowDict[field]%cardinality\n",
    "            if index < cardinality-1:\n",
    "                hashEnc[index] = 1\n",
    "            \n",
    "            X = np.append(X, hashEnc)\n",
    "    \n",
    "        # Set the actual value (Y) if the field is field_0\n",
    "        elif field == 'field_0':\n",
    "            Y = rowDict[field]\n",
    "            \n",
    "        # If the field is not categorical, then just use the existing value\n",
    "        else:\n",
    "            X = np.append(X, rowDict[field])\n",
    "    \n",
    "    return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe_fit(field, topN, df):\n",
    "\n",
    "    # Find the frequency of items in the category\n",
    "    fieldFreqRDD = df.rdd.map(lambda x: (x[field], 1)).\\\n",
    "                          reduceByKey(lambda x, y: x+y)\n",
    "\n",
    "    # Save the topN values in the dictionary associated with this field\n",
    "    validValuesDict = {}\n",
    "    index=0\n",
    "    for value in fieldFreqRDD.takeOrdered(topN, key=lambda x: -x[1]):\n",
    "        validValuesDict[value[0]] = index\n",
    "        index += 1\n",
    "\n",
    "    # Use the top N frequent values\n",
    "    valueDict[field] = validValuesDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe_transform(df):\n",
    "    oheRDD = df.rdd.map(lambda row: convertRowToArray(row))\n",
    "    return oheRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary for the values to create new fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "valueDict = {}\n",
    "cardinalityDict = {}\n",
    "featureHashingDict = {'field_12':10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the function to restrict the fields in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "oheFieldsAndSizes = [('field_2', 10), ('field_11', 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fieldAndSize in oheFieldsAndSizes:\n",
    "    cardinalityDict[fieldAndSize[0]] = fieldAndSize[1]\n",
    "    ohe_fit(fieldAndSize[0], fieldAndSize[1], testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "testRDD = ohe_transform(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0.]), 1),\n",
       " (array([2., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0.]), 2),\n",
       " (array([3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0.]), 2),\n",
       " (array([4., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0.]), 3),\n",
       " (array([5., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0.]), 3),\n",
       " (array([6., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1.]), 3),\n",
       " (array([7., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0.]), 4),\n",
       " (array([8., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0.]), 4),\n",
       " (array([9., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 0., 0.]), 4),\n",
       " (array([10.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), 4)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testRDD.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
