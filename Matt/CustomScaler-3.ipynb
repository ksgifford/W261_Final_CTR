{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Scaling Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do custom scaling due to the following limitations:\n",
    "\n",
    "1. MinMaxScaler in MLlib does not work for us, because it works on Vector objects\n",
    "2. MinMaxScaler in Scikit Learn works on Pandas DataFrames, but Pandas DataFrames do not scale\n",
    "3. Using UDF in Spark SQL is too cumbersome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data and convert to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.types import StructType, StructField, LongType, FloatType\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "app_name = \"scaling_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!head -n 10 data/train_100K.txt > data/train_10.txt\n",
    "#!head -n 20 data/train_100K.txt > data/test_10.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smallRDD = sc.textFile('data/train_10.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertNumber(idx, num):\n",
    "    if num != '':\n",
    "        return float(num)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallRDD2 = sc.parallelize([(1.0,1.0,3.0), (2.0,2.0,3.0), (2.0,3.0,1.0), (3.0,4.0,2.0), (3.0,5.0,2.0), (3.0,6.0,4.0), (4.0,7.0,4.0), (4.0,8.0,4.0), (4.0,9.0,4.0), (4.0,10.0,4.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallRDDRow = smallRDD2.take(1)\n",
    "numColumns = len(smallRDDRow[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "structFieldList = [StructField('field_1', FloatType(), True), StructField('field_2', FloatType(), True), StructField('field_11', FloatType(), True)]\n",
    "schema = StructType(structFieldList)\n",
    "testDF = spark.createDataFrame(smallRDD2, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+\n",
      "|field_1|field_2|field_11|\n",
      "+-------+-------+--------+\n",
      "|    1.0|    1.0|     3.0|\n",
      "|    2.0|    2.0|     3.0|\n",
      "|    2.0|    3.0|     1.0|\n",
      "|    3.0|    4.0|     2.0|\n",
      "|    3.0|    5.0|     2.0|\n",
      "|    3.0|    6.0|     4.0|\n",
      "|    4.0|    7.0|     4.0|\n",
      "|    4.0|    8.0|     4.0|\n",
      "|    4.0|    9.0|     4.0|\n",
      "|    4.0|   10.0|     4.0|\n",
      "+-------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the fields of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+\n",
      "|field_1|field_2|field_11|\n",
      "+-------+-------+--------+\n",
      "|    1.0|    1.0|     3.0|\n",
      "|    2.0|    2.0|     3.0|\n",
      "|    2.0|    3.0|     1.0|\n",
      "|    3.0|    4.0|     2.0|\n",
      "|    3.0|    5.0|     2.0|\n",
      "|    3.0|    6.0|     4.0|\n",
      "|    4.0|    7.0|     4.0|\n",
      "|    4.0|    8.0|     4.0|\n",
      "|    4.0|    9.0|     4.0|\n",
      "|    4.0|   10.0|     4.0|\n",
      "+-------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDF.select(['field_1', 'field_2', 'field_11']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define scaling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleRow(row):\n",
    "    rowDict = row.asDict()\n",
    "    \n",
    "    # Scale by subtracting the min, and dividing by the delta\n",
    "    for field in scaleDict.keys():\n",
    "        rowDict[field] = float(rowDict[field]-scaleDict[field][0])/scaleDict[field][1]\n",
    "\n",
    "    return pyspark.sql.Row(**rowDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleDataFrame_fit(fields, df):\n",
    "\n",
    "    # Note: Need to rename the 'summary' column, because using it in the filter statement tries to invoke the function\n",
    "    summaryDF = df.select(fields).summary(['mean', 'stddev']).withColumnRenamed('summary', 'summary_col').cache()\n",
    "    \n",
    "    meanRow = summaryDF.filter(summaryDF.summary_col == 'mean').first()\n",
    "    stddevRow = summaryDF.filter(summaryDF.summary_col == 'stddev').first()\n",
    "    \n",
    "    for field in fields:  \n",
    "        scaleDict[field] = (float(meanRow[field]), float(stddevRow[field]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleDataFrame_transform(df):\n",
    "    return df.rdd.map(scaleRow).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define an empty scaling dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaleDict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the selected fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['field_1', 'field_2', 'field_11']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "originalColumnOrder = testDF.columns\n",
    "originalColumnOrder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaleDataFrame_fit(['field_1','field_2','field_11'], testDF)\n",
    "testDF2 = scaleDataFrame_transform(testDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF2 = testDF2.select(originalColumnOrder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+\n",
      "|            field_1|             field_2|            field_11|\n",
      "+-------------------+--------------------+--------------------+\n",
      "|-1.8973665961010275| -1.4863010829205867|-0.09086737992230748|\n",
      "|-0.9486832980505138| -1.1560119533826787|-0.09086737992230748|\n",
      "|-0.9486832980505138| -0.8257228238447705| -1.9082149783684554|\n",
      "|                0.0|-0.49543369430686224| -0.9995411791453815|\n",
      "|                0.0| -0.1651445647689541| -0.9995411791453815|\n",
      "|                0.0|  0.1651445647689541|  0.8178064193007665|\n",
      "| 0.9486832980505138| 0.49543369430686224|  0.8178064193007665|\n",
      "| 0.9486832980505138|  0.8257228238447705|  0.8178064193007665|\n",
      "| 0.9486832980505138|  1.1560119533826787|  0.8178064193007665|\n",
      "| 0.9486832980505138|  1.4863010829205867|  0.8178064193007665|\n",
      "+-------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDF2.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
