{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Test #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from pyspark.sql.types import StructType, StructField, LongType\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql\n",
    "import pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "app_name = \"data_cleaning_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do this on the command line first: hadoop fs -mkdir /RddCheckPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.setCheckpointDir(\"hdfs://quickstart.cloudera:8020/RddCheckPoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainRDD = sc.textFile('data/train1percentsample.txt')\n",
    "#testRDD = sc.textFile('data/test1percentsample.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertNumber(idx, num):\n",
    "    if num != '':\n",
    "        if idx > 13:\n",
    "            return int(num, 16)\n",
    "        else:\n",
    "            return int(num)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainRDD2 = trainRDD.map(lambda x: [ConvertNumber(idx, num) for idx,num in enumerate(x.split('\\t'))]).cache()\n",
    "#testRDD2 = testRDD.map(lambda x: [ConvertNumber(idx, num) for idx,num in enumerate(x.split('\\t'))]).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainRDDRow = trainRDD2.take(1)\n",
    "numColumns = len(trainRDDRow[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "structFieldList = [StructField('field_' + str(num), LongType(), True) for num in range(numColumns)]\n",
    "schema = StructType(structFieldList)\n",
    "\n",
    "trainDF = spark.createDataFrame(trainRDD2, schema)\n",
    "#testDF = spark.createDataFrame(testRDD2, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleRow(row):\n",
    "    rowDict = row.asDict()\n",
    "    \n",
    "    # Scale by subtracting the min, and dividing by the delta\n",
    "    for field in scaleDict.keys():\n",
    "        temp = float(rowDict[field]-scaleDict[field][0])/scaleDict[field][1]\n",
    "        \n",
    "        # The test set may have data that is outside the range in the training set.\n",
    "        # Limit the range to 0-1.\n",
    "        rowDict[field] = min(max(temp, 0.0), 1.0)\n",
    "        \n",
    "    return pyspark.sql.Row(**rowDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleDataFrame_fit(fields, df):\n",
    "\n",
    "    # Note: Need to rename the 'summary' column, because using it in the filter statement tries to invoke the function\n",
    "    summaryDF = df.select(fields).summary(['min', 'max']).withColumnRenamed('summary', 'summary_col').cache()\n",
    "    \n",
    "    minRow = summaryDF.filter(summaryDF.summary_col == 'min').first()\n",
    "    maxRow = summaryDF.filter(summaryDF.summary_col == 'max').first()\n",
    "    \n",
    "    for field in fields:\n",
    "        min = int(minRow[field])\n",
    "        max = int(maxRow[field])    \n",
    "        scaleDict[field] = (min, max-min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleDataFrame_transform(df):\n",
    "    return df.rdd.map(scaleRow).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oheForRow(value):\n",
    "\n",
    "    # Encode the rare / unseen value\n",
    "    if value not in valueDict:\n",
    "        oheList = [0] * len(valueDict)\n",
    "        oheList.append(1)\n",
    "        return oheList\n",
    "\n",
    "    oheList = []\n",
    "\n",
    "    # Encode values that have been seen\n",
    "    for key in valueDict.keys():\n",
    "        if key == value:\n",
    "            oheList.append(1)\n",
    "        else:\n",
    "            oheList.append(0)\n",
    "    \n",
    "    # Last column is for rare / unseen. Set to 0.\n",
    "    oheList.append(0)\n",
    "    \n",
    "    return oheList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oheDataFrame_fit(field, topN, df):\n",
    "\n",
    "    # Find the frequency of items in the category\n",
    "    fieldFreqRDD = df.rdd.map(lambda x: (x[field], 1)).\\\n",
    "                          reduceByKey(lambda x, y: x+y)\n",
    "\n",
    "    # Use the top N frequent values\n",
    "    valueCountList = fieldFreqRDD.takeOrdered(topN, key=lambda x: -x[1])\n",
    "    \n",
    "    # Add those values to a dictionary for OHE\n",
    "    for pair in valueCountList:\n",
    "        valueDict[pair[0]] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oheDataFrame_transform(field, df):\n",
    "\n",
    "    # Create a new RDD with the encoded values\n",
    "    oheRDD = df.rdd.map(lambda row: oheForRow(row[field]))\n",
    "\n",
    "    # Create a new DataFrame for the OHE RDD\n",
    "    structFieldList = [StructField(field + '_' + str(num), LongType(), True) for num in range(len(valueDict))]\n",
    "    structFieldList.append(StructField(field + '_UnkwnRare', LongType(), True))\n",
    "    schema = StructType(structFieldList)\n",
    "    oheDF = spark.createDataFrame(oheRDD, schema)\n",
    "    \n",
    "    # Add an index column\n",
    "    df = df.withColumn('id', pyspark.sql.functions.monotonically_increasing_id())\n",
    "    oheDF = oheDF.withColumn('id', pyspark.sql.functions.monotonically_increasing_id())\n",
    "\n",
    "    # Join the original DataFrame with the OHE DataFrame\n",
    "    #updatedDF = df.join(oheDF, df.id == oheDF.id, 'inner').drop(oheDF.id)\n",
    "    updatedDF = df.join(oheDF, ['id'])\n",
    "    \n",
    "    # Drop the original field that was OHE\n",
    "    updatedDF = updatedDF.drop(field)\n",
    "    updatedDF = updatedDF.drop('id')\n",
    "\n",
    "    return updatedDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Optimize this to do all columns at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputeWithMean(field, df):\n",
    "    fieldMean = df.rdd.map(lambda row: row[field]).filter(lambda x: x != None).mean()\n",
    "    return df.fillna(fieldMean, [field])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started processing the fields...\n"
     ]
    }
   ],
   "source": [
    "print('Started processing the fields...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has about 42% null values.  Drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_1')\n",
    "#testDF = testDF.drop('field_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No null values, so scale this field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has about 23% null values.  Drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_3')\n",
    "#testDF = testDF.drop('field_3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has about 25% null values.  Drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_4')\n",
    "#testDF = testDF.drop('field_4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has about 3% null values.  Impute with the mean and scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = imputeWithMean('field_5', trainDF)\n",
    "#testDF = imputeWithMean('field_5', testDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has about 22% null values. Drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_6')\n",
    "#testDF = testDF.drop('field_6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has about 4% null values. Impute with the mean and scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = imputeWithMean('field_7', trainDF)\n",
    "#testDF = imputeWithMean('field_7', testDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has < 1% null values. Impute with the mean and scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = imputeWithMean('field_8', trainDF)\n",
    "#testDF = imputeWithMean('field_8', testDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 4% null values. Impute with the mean and scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = imputeWithMean('field_9', trainDF)\n",
    "#testDF = imputeWithMean('field_9', testDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 42% null values. Drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_10')\n",
    "#testDF = testDF.drop('field_10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 4% null values. Impute with the mean and scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = imputeWithMean('field_11', trainDF)\n",
    "#testDF = imputeWithMean('field_11', testDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 77% null values. Drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_12')\n",
    "#testDF = testDF.drop('field_12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 25% null values. Drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_13')\n",
    "#testDF = testDF.drop('field_13')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 0% null values. It has a modest amount of distinct values, so OHE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 0% null values. It has a modest amount of distinct values, so OHE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 4% null values. It has a large amount of distinct values, so drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_16')\n",
    "#testDF = testDF.drop('field_16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 4% null values. It has a large amount of distinct values, so drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_17')\n",
    "#testDF = testDF.drop('field_17')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 0% null values. It has a modest amount of distinct values, so OHE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 12% null values, so delete those observations (NOTE: this is categorical, so we can't impute).\n",
    "It has a modest amount of distinct values, so OHE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 0% null values. It has a modest amount of distinct values, so OHE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 0% null values. It has a modest amount of distinct values, so OHE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 0% null values. It has a small amount of distinct values, so OHE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 0% null values. It has a large amount of distinct values, so drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_23')\n",
    "#testDF = testDF.drop('field_23')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 0% null values. It has a modest amount of distinct values, so OHE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 4% null values. It has a large amount of distinct values, so drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_25')\n",
    "#testDF = testDF.drop('field_25')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 0% null values. It has a modest amount of distinct values, so OHE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 0% null values. It has a small amount of distinct values, so OHE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 0% null values. It has a modest amount of distinct values, so OHE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 4% null values. It has a large amount of distinct values, so drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_29')\n",
    "#testDF = testDF.drop('field_29')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 0% null values. It has a small amount of distinct values, so OHE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 0% null values. It has a modest amount of distinct values, so OHE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 48% null values, so drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_32')\n",
    "#testDF = testDF.drop('field_32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 48% null values, so drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_33')\n",
    "#testDF = testDF.drop('field_33')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 4% null values. It has a large amount of distinct values, so drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_34')\n",
    "#testDF = testDF.drop('field_34')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 74% null values, so drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_35')\n",
    "#testDF = testDF.drop('field_35')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 0% null values. It has a small amount of distinct values, so OHE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 4% null values. It has a large amount of distinct values, so drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_37')\n",
    "#testDF = testDF.drop('field_37')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 48% null values, so drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_38')\n",
    "#testDF = testDF.drop('field_38')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field 39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field has 48% null values, so drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop('field_39')\n",
    "#testDF = testDF.drop('field_39')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Finished processing the fields\n"
     ]
    }
   ],
   "source": [
    "print('...Finished processing the fields')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete null observations in a batch (this is for categorical values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.dropna(how='any', subset=['field_19'])\n",
    "#testDF = testDF.dropna(how='any', subset=['field_19'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Do Imputation In a Batch (this is for numerical values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do Scaling (all columns at once)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note that this is being done after observations have been deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started scaling fields...\n"
     ]
    }
   ],
   "source": [
    "print('Started scaling fields...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaleDict = {}\n",
    "scaleDataFrame_fit(['field_2','field_5','field_7','field_8','field_9','field_11'], trainDF)\n",
    "trainDF = scaleDataFrame_transform(trainDF)\n",
    "#testDF = scaleDataFrame_transform(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|             field_2|             field_5|             field_7|             field_8|             field_9|            field_11|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|2.563445270443476E-4|5.197465732413659E-4|1.624827362092777...|8.710801393728223E-4| 1.44102601051949E-4|0.006944444444444444|\n",
      "|1.538067162266085...|1.746409230759885...|5.686895767324722E-4|2.903600464576074...|                 0.0|0.006944444444444444|\n",
      "|1.538067162266085...|1.898270902999875...|0.005361930294906166|0.005226480836236934|0.005547950140500036|0.027777777777777776|\n",
      "| 0.04301461163804153|5.918808675553611E-4|0.002031034202615972|2.903600464576074...|0.028244109806182003| 0.04861111111111111|\n",
      "|5.126890540886952E-5|0.002833359149817614|8.124136810463888E-5|                 0.0|                 0.0|0.006944444444444444|\n",
      "|5.126890540886952E-5|0.016551403657436514|                 0.0|8.710801393728223E-4| 5.76410404207796E-4|                 0.0|\n",
      "|2.050756216354780...|7.593083611999502E-7|0.001299861889674...|  0.0078397212543554|0.002161539015779...|0.013888888888888888|\n",
      "|0.001486798256857...|0.002135175111694...|4.062068405231944E-4|0.006968641114982578|0.002449744217883...|0.006944444444444444|\n",
      "|1.538067162266085...| 0.35482973269308454|0.001299861889674...|0.004355400696864111|0.007565386555227322|0.013888888888888888|\n",
      "|1.538067162266085...|5.766947003313621E-4|5.686895767324722E-4|0.003484320557491289|6.484617047337705E-4|0.006944444444444444|\n",
      "|0.002563445270443476|                 0.0|8.124136810463888E-5|                 0.0|                 0.0|0.006944444444444444|\n",
      "|0.018969495001281724|1.833729692297879...|4.062068405231944E-4|8.710801393728223E-4|5.043591036818215E-4|0.013888888888888888|\n",
      "|2.050756216354780...|3.796541805999750...|6.499309448371111E-4|0.012775842044134728|0.040348728294545715|0.020833333333333332|\n",
      "|0.003486285567803...|  0.1081866349579495|                 0.0|0.001161440185830...|4.323078031558469...|                 0.0|\n",
      "|0.002307100743399...|3.796541805999750...|4.062068405231944E-4|0.013937282229965157|0.002737949419987...|0.006944444444444444|\n",
      "|5.126890540886952E-5|0.001123396720395...|3.249654724185555...|0.001451800232288...|0.001296923409467541|0.006944444444444444|\n",
      "|1.538067162266085...| 0.00974382454509836|                 0.0|0.001742160278745...|8.646156063116939E-4|                 0.0|\n",
      "|0.001486798256857...|3.135943531755794...|0.003493378828499472|8.710801393728223E-4| 0.03314359824194827|              0.0625|\n",
      "|0.004204050243527301|0.060180123129443854|                 0.0|0.002903600464576...| 2.88205202103898E-4|                 0.0|\n",
      "|1.025378108177390...|4.552053625393701...|2.437241043139166...|0.002322880371660...| 5.76410404207796E-4|0.006944444444444444|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.select(['field_2','field_5','field_7','field_8','field_9','field_11']).show(n=20)\n",
    "#testDF.select(['field_2','field_5','field_7','field_8','field_9','field_11']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Finished scaling fields\n"
     ]
    }
   ],
   "source": [
    "print('...Finished scaling fields')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create OHE fields (one column at a time to control for number of distinct values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started OHE fields...\n"
     ]
    }
   ],
   "source": [
    "print('Started OHE fields...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "oheFieldsAndSizes = [('field_14',10),('field_15',10),('field_18',10),('field_19',10),('field_20',10),('field_21',10),('field_22',3),('field_24',10),('field_26',10),('field_27',10),('field_28',10),('field_30',10),('field_31',10),('field_36',10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fieldAndSize in oheFieldsAndSizes:\n",
    "    valueDict = {}\n",
    "    fieldToEncode = fieldAndSize[0]\n",
    "    oheDataFrame_fit(fieldToEncode, fieldAndSize[1], trainDF)\n",
    "    trainDF = oheDataFrame_transform(fieldToEncode, trainDF)\n",
    "    #testDF = oheDataFrame_transform(fieldToEncode, testDF)\n",
    "    \n",
    "    trainDF.cache()\n",
    "    #testDF.cache()\n",
    "\n",
    "    #trainDF.checkpoint(eager=True)\n",
    "    #testDF.checkpoint(eager=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Finished OHE fields\n"
     ]
    }
   ],
   "source": [
    "print('...Finished OHE fields')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save cleaned DataFrame to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started saving the DataFrames...\n"
     ]
    }
   ],
   "source": [
    "print('Started saving the DataFrames...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF.write.csv(path='data/train1percentsample_cleaned_tmp.txt', header=False, sep='\\t')\n",
    "#testDF.write.csv(path='data/test1percentsample_cleaned_tmp.txt', header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf data/train1percentsample_cleaned.txt\n",
    "#!rm -rf data/test1percentsample_cleaned.txt\n",
    "!mv data/train1percentsample_cleaned_tmp.txt data/train1percentsample_cleaned.txt\n",
    "#!mv data/test1percentsample_cleaned_tmp.txt data/test1percentsample_cleaned.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Finished saving the DataFrames\n"
     ]
    }
   ],
   "source": [
    "print('...Finished saving the DataFrames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total time was: 442.5566794872284 seconds\n"
     ]
    }
   ],
   "source": [
    "print('The total time was: {} seconds'.format(time.time() - startTime))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
