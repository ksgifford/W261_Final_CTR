{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from pyspark.sql.types import StructType, StructField, LongType, FloatType\n",
    "import pyspark.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"w261_final\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data, Covert hex to int and creat DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the data\n",
    "sampleRDD = sc.textFile('data/trainpt1percentsample.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data and concer hext to int\n",
    "def ConvertNumber(idx, num):\n",
    "    if num != '':\n",
    "        if idx > 13:\n",
    "            return int(num, 16)\n",
    "        elif idx == 0:\n",
    "            return int(num)\n",
    "        else:\n",
    "            return float(num)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "trainRDD = sampleRDD.map(lambda x: [ConvertNumber(idx, num) for idx,num in enumerate(x.split('\\t'))]).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create schema for df \n",
    "structFieldList = [StructField('field_0', LongType(), True)] +\\\n",
    "                  [StructField('field_' + str(num), FloatType(), True) for num in range(1, 14)] +\\\n",
    "                  [StructField('field_' + str(num), LongType(), True) for num in range(14,40)]\n",
    "schema = StructType(structFieldList)\n",
    "#and create dfs\n",
    "trainRDD, testRDD =(trainRDD.randomSplit([0.8,0.2], seed = 1))\n",
    "trainDF = spark.createDataFrame(trainRDD, schema)\n",
    "testDF = spark.createDataFrame(testRDD, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start the clock to see how long it takes to run following celss\n",
    "startTime = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop unnecessary Cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_to_drop = ['field_1','field_3','field_4','field_6','field_10',\n",
    "    'field_12','field_13','field_16','field_17','field_19','field_20',\n",
    "    'field_23','field_25','field_29','field_32','field_33','field_34',\n",
    "    'field_35','field_37','field_38','field_39']\n",
    "\n",
    "for f in fields_to_drop:\n",
    "    trainDF = trainDF.drop(f)\n",
    "    testDF = testDF.drop(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total time was: 1.217175006866455 seconds\n"
     ]
    }
   ],
   "source": [
    "new_time = time.time()\n",
    "print('The total time was: {} seconds'.format(time.time() - startTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute Mean for Continuous Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputeWithMean(field, traindf,testdf):\n",
    "    fieldMean = traindf.rdd.map(lambda row: row[field]).filter(lambda x: x != None).mean()\n",
    "    return traindf.fillna(fieldMean, [field]),testdf.fillna(fieldMean, [field])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_to_impute = ['field_5','field_7','field_8','field_9','field_11']\n",
    "\n",
    "for m in fields_to_impute:\n",
    "    trainDF,testDF = imputeWithMean(m, trainDF,testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total time was: 18.358896493911743 seconds\n"
     ]
    }
   ],
   "source": [
    "print('The total time was: {} seconds'.format(time.time() - new_time))\n",
    "new_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Categorical Observations that are Null\n",
    "- Currently, dropping all categorical columns that have any Nulls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Continusous Data\n",
    "- Should we convert one-hot and then scale all variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleRow(row):\n",
    "    rowDict = row.asDict()\n",
    "    \n",
    "    # Scale by subtracting the meann, and dividing by the stdDev\n",
    "    for field in scaleDict.keys():\n",
    "        rowDict[field] = float(rowDict[field]-scaleDict[field][0])/scaleDict[field][1]\n",
    "\n",
    "    return pyspark.sql.Row(**rowDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleDataFrame_fit(fields, df):\n",
    "\n",
    "    # Note: Need to rename the 'summary' column, because using it in the filter statement tries to invoke the function\n",
    "    summaryDF = df.select(fields).summary(['mean', 'stddev']).withColumnRenamed('summary', 'summary_col').cache()\n",
    "    \n",
    "    meanRow = summaryDF.filter(summaryDF.summary_col == 'mean').first()\n",
    "    stddevRow = summaryDF.filter(summaryDF.summary_col == 'stddev').first()\n",
    "    \n",
    "    for field in fields:  \n",
    "        scaleDict[field] = (float(meanRow[field]), float(stddevRow[field]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleDataFrame_transform(df):\n",
    "    return df.rdd.map(scaleRow).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dict to hold key - field value - (mean, stdDev)\n",
    "scaleDict = {}\n",
    "\n",
    "# Save original column order\n",
    "originalColumnOrderTrain = trainDF.columns\n",
    "\n",
    "scaleDataFrame_fit(['field_2','field_5','field_7','field_8','field_9','field_11'], trainDF)\n",
    "trainDF = scaleDataFrame_transform(trainDF)\n",
    "testDF = scaleDataFrame_transform(testDF)\n",
    "\n",
    "# Reset the original column order\n",
    "trainDF = trainDF.select(originalColumnOrderTrain)\n",
    "testDF = testDF.select(originalColumnOrderTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total time was: 6.388637542724609 seconds\n"
     ]
    }
   ],
   "source": [
    "print('The total time was: {} seconds'.format(time.time() - new_time))\n",
    "new_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot Categorical Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertRowToArray(row):\n",
    "    rowDict = row.asDict()\n",
    "    \n",
    "    X = np.array([])\n",
    "    \n",
    "    # Iterate over fields in the row\n",
    "    for field in rowDict.keys():\n",
    "        \n",
    "        # If the field is categorical\n",
    "        if field in valueDict:\n",
    "            \n",
    "            if rowDict[field] not in valueDict[field]:\n",
    "                \n",
    "                # If the value is not found in the categories for that field (rare/unknown),\n",
    "                # then the encoding is all zeros\n",
    "                \n",
    "                X = np.append(X, np.zeros(cardinalityDict[field]))\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # If the value is found in the categories for that field      \n",
    "                ohe = np.zeros(cardinalityDict[field])\n",
    "                \n",
    "                # Look up the value in the dictionary for this category (it is an index)\n",
    "                index = valueDict[field][rowDict[field]]\n",
    "                ohe[index] = 1\n",
    "                X = np.append(X, ohe)\n",
    "    \n",
    "        # Set the actual value (Y) if the field is field_0\n",
    "        elif field == 'field_0':\n",
    "            Y = rowDict[field]\n",
    "            \n",
    "        # If the field is not categorical, then just use the existing value\n",
    "        else:\n",
    "            X = np.append(X, rowDict[field])\n",
    "    \n",
    "    return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe_fit(field, topN, df):\n",
    "\n",
    "    # Find the frequency of items in the category\n",
    "    fieldFreqRDD = df.rdd.map(lambda x: (x[field], 1)).\\\n",
    "                          reduceByKey(lambda x, y: x+y)\n",
    "\n",
    "    # Save the topN values in the dictionary associated with this field\n",
    "    validValuesDict = {}\n",
    "    index=0\n",
    "    for value in fieldFreqRDD.takeOrdered(topN, key=lambda x: -x[1]):\n",
    "        validValuesDict[value[0]] = index\n",
    "        index += 1\n",
    "\n",
    "    # Use the top N frequent values\n",
    "    valueDict[field] = validValuesDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe_transform(df):\n",
    "    oheRDD = df.rdd.map(lambda row: convertRowToArray(row))\n",
    "    return oheRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dicts to hold top category values\n",
    "valueDict = {}\n",
    "cardinalityDict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set number of categories to keep per field\n",
    "oheFieldsAndSizes = [('field_14', 10), ('field_15', 10), ('field_18', 10), ('field_21', 10), ('field_22', 3), ('field_24', 10), ('field_26', 10), ('field_27', 10), ('field_28', 10), ('field_30', 10), ('field_31', 10), ('field_36', 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through fields to update dicts\n",
    "for fieldAndSize in oheFieldsAndSizes:\n",
    "\n",
    "    fieldToEncode = fieldAndSize[0]\n",
    "    cardinalityDict[fieldToEncode] = fieldAndSize[1]\n",
    "    \n",
    "    ohe_fit(fieldToEncode, fieldAndSize[1], trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert categorical vals to OHE\n",
    "trainRDD = ohe_transform(trainDF)\n",
    "testRDD = ohe_transform(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total time was: 33.7352979183197 seconds\n",
      "The total time was: 59.70392990112305 seconds\n"
     ]
    }
   ],
   "source": [
    "print('The total time was: {} seconds'.format(time.time() - new_time))\n",
    "new_time = time.time()\n",
    "print('The total time was: {} seconds'.format(time.time() - startTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logloss(dataRDD, W):\n",
    "    \"\"\"\n",
    "    Calculates the mean log loss from an RDD of (np.array features, labels)\n",
    "    and a weights array W\n",
    "    \"\"\"\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1]))\n",
    "    loss = (augmentedData.\n",
    "            map(lambda x: (np.clip(1 / (1 + np.exp(-1*W @ x[0])), 1e-15, 1.0 - 1e-15), x[1])).\n",
    "            map(lambda x: -1*np.log(x[0]) if x[1]==1.0 else -1*np.log(1-x[0])).mean())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GDUpdate(dataRDD, W, learningRate = 0.05, regType = None, regParam = 0):\n",
    "    \"\"\"\n",
    "    Perform one gradient descent step/update.\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    Returns:\n",
    "        new_model - (array) updated coefficients, bias at index 0\n",
    "    \"\"\"\n",
    "    # add a bias 'feature' of 1 at index 0\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1])).cache()\n",
    "    \n",
    "    # use negative log likelihood to keep it descent\n",
    "    grad = augmentedData.map(lambda x: -x[0]*(x[1] - (1/(1+np.exp(-1*np.dot(W, x[0])))))).mean()\n",
    "    if regType=='l1':\n",
    "        grad[1:] += 2 * regParam * W[1:]\n",
    "    elif regType=='l2':\n",
    "        grad[1:] += regParam * np.sign(W[1:])\n",
    "\n",
    "    new_model = W - learningRate * grad \n",
    "   \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(line):\n",
    "    \"\"\"\n",
    "    Map tab separated record to tuple of (39 features, label)\n",
    "    \"\"\"\n",
    "    parsed = np.array([ConvertNumber(idx, num) for idx,num in enumerate(line.split('\\t'))])\n",
    "    features, label = parsed[1:], parsed[0]\n",
    "    return (features, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model from final step of prev run\n",
    "W = np.array([-0.714, 0.372, -0.016, 0.3, 0.367, 0.353, 0.568, -0.086, 0.286, 0.662, 0.57, 0.103, 0.01, 0.108, 0.201, 0.507, 0.556, -0.003, 0.128, 0.712, 0.279, 0.086, 0.653, 0.526, 0.772, 0.02, 0.859, -0.35, 0.344, 0.422, 0.004, 0.639, 0.452, 0.639, 0.286, 0.53, 0.824, -0.361, -0.087, 0.189, 0.361, 0.889, 0.51, 0.806, 0.497, 0.116, 0.423, -0.725, 0.693, 0.006, 0.693, 0.761, 0.582, 0.744, 0.311, 0.273, 0.202, 0.506, 0.373, 0.449, 0.775, 0.392, 0.497, 0.779, 0.522, 0.201, 0.133, 0.547, 0.286, 0.271, 0.376, 0.036, -0.006, 0.787, -0.001, 0.64, 0.222, 0.593, 0.627, 0.899, 0.168, 0.675, 0.675, 0.806, 0.056, 0.08, 0.309, 0.757, 0.742, 0.099, -0.295, 0.615, 0.268, 0.363, 0.439, 0.22, 0.792, 0.669, 0.504, 0.329, 0.888, 0.824, 0.442, 0.423, 0.307, 0.858, 0.08, 0.589, -0.008, 0.929, 0.407, 0.26, 0.69, 0.238, 0.58, 0.096, 0.77, 0.074, 0.807, 0.094])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Model: [-0.714, 0.372, -0.016, 0.3, 0.367, 0.353, 0.568, -0.086, 0.286, 0.662, 0.57, 0.103, 0.01, 0.108, 0.201, 0.507, 0.556, -0.003, 0.128, 0.712, 0.279, 0.086, 0.653, 0.526, 0.772, 0.02, 0.859, -0.35, 0.344, 0.422, 0.004, 0.639, 0.452, 0.639, 0.286, 0.53, 0.824, -0.361, -0.087, 0.189, 0.361, 0.889, 0.51, 0.806, 0.497, 0.116, 0.423, -0.725, 0.693, 0.006, 0.693, 0.761, 0.582, 0.744, 0.311, 0.273, 0.202, 0.506, 0.373, 0.449, 0.775, 0.392, 0.497, 0.779, 0.522, 0.201, 0.133, 0.547, 0.286, 0.271, 0.376, 0.036, -0.006, 0.787, -0.001, 0.64, 0.222, 0.593, 0.627, 0.899, 0.168, 0.675, 0.675, 0.806, 0.056, 0.08, 0.309, 0.757, 0.742, 0.099, -0.295, 0.615, 0.268, 0.363, 0.439, 0.22, 0.792, 0.669, 0.504, 0.329, 0.888, 0.824, 0.442, 0.423, 0.307, 0.858, 0.08, 0.589, -0.008, 0.929, 0.407, 0.26, 0.69, 0.238, 0.58, 0.096, 0.77, 0.074, 0.807, 0.094]\n",
      "Initial Train Loss = 0.8254698020736607\n",
      "Initial Held-Out Loss = 0.8245669609649684\n",
      "-----------------------------------------------------\n",
      "iteration # 0\n",
      "New Model: [-0.73, 0.358, -0.012, 0.286, 0.348, 0.335, 0.554, -0.082, 0.273, 0.65, 0.559, 0.092, -0.0, 0.098, 0.191, 0.497, 0.546, 0.006, 0.117, 0.701, 0.268, 0.075, 0.642, 0.515, 0.761, 0.01, 0.848, -0.348, 0.33, 0.41, -0.007, 0.628, 0.442, 0.629, 0.276, 0.52, 0.814, -0.359, -0.08, 0.177, 0.35, 0.878, 0.5, 0.796, 0.487, 0.106, 0.413, -0.727, 0.678, -0.004, 0.682, 0.75, 0.571, 0.733, 0.301, 0.263, 0.192, 0.496, 0.363, 0.439, 0.764, 0.381, 0.486, 0.768, 0.511, 0.191, 0.123, 0.537, 0.276, 0.261, 0.357, 0.023, 0.002, 0.777, 0.009, 0.63, 0.212, 0.583, 0.617, 0.889, 0.157, 0.665, 0.664, 0.796, 0.046, 0.07, 0.299, 0.747, 0.732, 0.089, -0.288, 0.602, 0.256, 0.351, 0.428, 0.209, 0.781, 0.658, 0.493, 0.319, 0.877, 0.813, 0.431, 0.413, 0.297, 0.847, 0.069, 0.578, 0.001, 0.919, 0.39, 0.247, 0.679, 0.226, 0.568, 0.085, 0.759, 0.064, 0.797, 0.084]\n",
      "Train Loss = 0.8071333225853421\n",
      "Held-Out Loss = 0.8065219699755678\n",
      "-----------------------------------------------------\n",
      "iteration # 1\n",
      "New Model: [-0.746, 0.344, -0.007, 0.273, 0.329, 0.317, 0.54, -0.078, 0.26, 0.638, 0.548, 0.082, 0.009, 0.087, 0.181, 0.486, 0.536, -0.005, 0.106, 0.689, 0.256, 0.065, 0.631, 0.505, 0.75, -0.001, 0.838, -0.345, 0.316, 0.399, 0.003, 0.618, 0.431, 0.618, 0.266, 0.51, 0.804, -0.356, -0.073, 0.166, 0.339, 0.867, 0.489, 0.785, 0.477, 0.096, 0.403, -0.727, 0.664, 0.006, 0.67, 0.739, 0.561, 0.723, 0.29, 0.253, 0.181, 0.485, 0.353, 0.429, 0.752, 0.37, 0.476, 0.758, 0.501, 0.18, 0.112, 0.526, 0.266, 0.25, 0.339, 0.01, -0.009, 0.766, -0.002, 0.619, 0.201, 0.572, 0.607, 0.879, 0.147, 0.654, 0.654, 0.785, 0.036, 0.06, 0.289, 0.736, 0.721, 0.079, -0.281, 0.589, 0.244, 0.339, 0.416, 0.198, 0.77, 0.647, 0.481, 0.309, 0.865, 0.803, 0.42, 0.402, 0.286, 0.836, 0.059, 0.568, -0.01, 0.909, 0.374, 0.233, 0.668, 0.215, 0.556, 0.075, 0.749, 0.054, 0.787, 0.074]\n",
      "Train Loss = 0.7896187390123839\n",
      "Held-Out Loss = 0.7893528855984498\n"
     ]
    }
   ],
   "source": [
    "# train the logistic regression\n",
    "###W = np.random.rand(120,) #only use if don't want to use model from prev run\n",
    "print(f\"Initial Model: {[round(w,3) for w in W]}\")\n",
    "print(f\"Initial Train Loss = {logloss(trainRDD,W)}\")\n",
    "print(f\"Initial Held-Out Loss = {logloss(testRDD,W)}\")\n",
    "n_iterations = 2\n",
    "for i in range(n_iterations):\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print(\"iteration # {}\".format(i))\n",
    "    W = GDUpdate(trainRDD, W, learningRate = .1, regType = 'l2', regParam = 0.1)\n",
    "    print(f\"New Model: {[round(w,3) for w in W]}\")\n",
    "    print(f\"Train Loss = {logloss(trainRDD,W)}\")\n",
    "    print(f\"Held-Out Loss = {logloss(testRDD,W)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
