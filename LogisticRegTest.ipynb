{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATAFILE = 'trainpt1percentsample.txt'\n",
    "FIELDS = ['Label'] + ['I'+str(i) for i in range(1,14)] + ['C'+str(i) for i in range(1,27)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3fdba71cb344>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# start Spark Session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mapp_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"hw3_notebook\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmaster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"local[*]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapp_name\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"hw3_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sc.textFile(DATAFILE)\n",
    "trainRDD, heldOutRDD = data.randomSplit([0.8,0.2], seed = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ConvertNumber(idx, num):\n",
    "    \"\"\"\n",
    "    convert hashes to ints\n",
    "    \"\"\"\n",
    "    if num != '':\n",
    "        if idx > 13:\n",
    "            return int(num, 16)\n",
    "        else:\n",
    "            return int(num)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def parse(line):\n",
    "    \"\"\"\n",
    "    Map tab separated record to tuple of features and label\n",
    "    \"\"\"\n",
    "    parsed = np.array([ConvertNumber(idx, num) for idx,num in enumerate(line.split('\\t'))])\n",
    "    features, label = parsed[1:], parsed[0]\n",
    "    return (features, label)\n",
    "\n",
    "def logloss(dataRDD, W):\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1]))\n",
    "    loss = (augmentedData.\n",
    "            map(lambda x: (np.clip(1 / (1 + np.exp(-1*W @ x[0])), 1e-15, 1.0 - 1e-15), x[1])).\n",
    "            map(lambda x: -1*np.log(x[0]) if x[1]==1.0 else -1*np.log(1-x[0])).mean())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainRDDCached = trainRDD.map(parse).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test a regression on a constant\n",
    "onefeature_trainrdd = trainRDDCached.map(lambda x: (np.array([]),x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5663501091618366"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline with best possible constant value\n",
    "BASELINE = np.array([-1*np.log(1 / onefeature_trainrdd.map(lambda x: x[1]).mean() - 1)])\n",
    "logloss(onefeature_trainrdd, BASELINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# can we get there with gradient descent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GDUpdate(dataRDD, W, learningRate = 0.0001):\n",
    "    \"\"\"\n",
    "    Perform one gradient descent step/update.\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    Returns:\n",
    "        new_model - (array) updated coefficients, bias at index 0\n",
    "    \"\"\"\n",
    "    # add a bias 'feature' of 1 at index 0\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1])).cache()\n",
    "    \n",
    "    grad = augmentedData.map(lambda x: x[0]*(x[1] - (1/(1+np.exp(-1*np.dot(W, x[0])))))).sum()\n",
    "    new_model = W + learningRate * grad \n",
    "   \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Loss = 0.5663501091618366\n",
      "Target Model = [-1.07904134]\n",
      "----------\n",
      "STEP: 1\n",
      "Loss: 0.5768600254954287\n",
      "Model: [-0.754]\n",
      "----------\n",
      "STEP: 2\n",
      "Loss: 0.5669876111570928\n",
      "Model: [-0.998]\n",
      "----------\n",
      "STEP: 3\n",
      "Loss: 0.5664033829314044\n",
      "Model: [-1.055]\n",
      "----------\n",
      "STEP: 4\n",
      "Loss: 0.5663548984651946\n",
      "Model: [-1.072]\n",
      "----------\n",
      "STEP: 5\n",
      "Loss: 0.5663505487092614\n",
      "Model: [-1.077]\n"
     ]
    }
   ],
   "source": [
    "nSteps = 5\n",
    "model = np.array([1])\n",
    "print(f\"Target Loss = {logloss(onefeature_trainrdd,BASELINE)}\")\n",
    "print(f\"Target Model = {BASELINE}\")\n",
    "for idx in range(nSteps):\n",
    "    print(\"----------\")\n",
    "    print(f\"STEP: {idx+1}\")\n",
    "    model = GDUpdate(onefeature_trainrdd, model)\n",
    "    loss = logloss(onefeature_trainrdd, model)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    print(f\"Model: {[round(w,3) for w in model]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.0\n",
      "13910.0\n"
     ]
    }
   ],
   "source": [
    "print(trainRDDCached.map(lambda x: x[0][1]).min())\n",
    "print(trainRDDCached.map(lambda x: x[0][1]).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test a regression on a constant and one variable\n",
    "meanfeature = trainRDDCached.map(lambda x: x[0][1]).mean()\n",
    "varfeature = trainRDDCached.map(lambda x: x[0][1]).variance()\n",
    "twofeature_trainrdd = trainRDDCached.map(lambda x: (np.array(([x[0][1]] - meanfeature)/varfeature**.5),x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5663501091618366"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# expand baseline\n",
    "BASELINE = np.array([-1*np.log(1 / onefeature_trainrdd.map(lambda x: x[1]).mean() - 1),0])\n",
    "logloss(twofeature_trainrdd, BASELINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE Loss = 0.5663501091618366\n",
      "----------\n",
      "STEP: 1\n",
      "Loss: 0.5655555218839737\n",
      "Model: [-1.079, 0.068]\n",
      "----------\n",
      "STEP: 2\n",
      "Loss: 0.5655148482350966\n",
      "Model: [-1.08, 0.084]\n",
      "----------\n",
      "STEP: 3\n",
      "Loss: 0.5655134289907428\n",
      "Model: [-1.08, 0.087]\n",
      "----------\n",
      "STEP: 4\n",
      "Loss: 0.5655133728075366\n",
      "Model: [-1.081, 0.088]\n",
      "----------\n",
      "STEP: 5\n",
      "Loss: 0.5655133698654112\n",
      "Model: [-1.081, 0.088]\n"
     ]
    }
   ],
   "source": [
    "nSteps = 5\n",
    "model = BASELINE\n",
    "print(f\"BASELINE Loss = {logloss(twofeature_trainrdd,BASELINE)}\")\n",
    "for idx in range(nSteps):\n",
    "    print(\"----------\")\n",
    "    print(f\"STEP: {idx+1}\")\n",
    "    model = GDUpdate(twofeature_trainrdd, model)\n",
    "    loss = logloss(twofeature_trainrdd, model)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    print(f\"Model: {[round(w,3) for w in model]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
