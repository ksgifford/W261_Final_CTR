{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"hw4_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make up some fake data. If < 5 then 0 if > 5 then 1\n",
    "X = np.random.randint(1,10,100)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 9, 8, 1, 1, 9, 6, 2, 2, 9, 4, 2, 2, 5, 2, 6, 4, 2, 9, 5, 7, 5,\n",
       "       2, 6, 9, 7, 8, 7, 2, 8, 5, 5, 4, 6, 9, 8, 6, 9, 6, 1, 4, 4, 9, 6,\n",
       "       7, 5, 4, 9, 3, 3, 1, 8, 4, 7, 9, 6, 4, 4, 7, 3, 7, 8, 5, 9, 2, 6,\n",
       "       2, 6, 8, 5, 2, 1, 5, 7, 8, 6, 2, 3, 9, 1, 9, 4, 8, 6, 7, 8, 9, 5,\n",
       "       4, 1, 8, 9, 2, 3, 5, 9, 4, 7, 6, 9])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.where(X < 5,0,1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "       1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "       0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conbine for spark\n",
    "combined = np.vstack((X,y)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feed into spark\n",
    "dummyRDD = sc.parallelize(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... held out 31 records for evaluation and assigned 69 for training.\n"
     ]
    }
   ],
   "source": [
    "# Generate 80/20 (pseudo)random train/test split - RUN THIS CELL AS IS\n",
    "trainRDD, heldOutRDD = dummyRDD.randomSplit([0.8,0.2], seed = 1)\n",
    "print(f\"... held out {heldOutRDD.count()} records for evaluation and assigned {trainRDD.count()} for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss based on sigmoid activation\n",
    "def logloss(dataRDD, W):\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1]))\n",
    "    loss = (augmentedData.\n",
    "            map(lambda x: (np.clip(1 / (1 + np.exp(-1*W @ x[0])), 1e-15, 1.0 - 1e-15), x[1])).\n",
    "            map(lambda x: -1*np.log(x[0]) if x[1]==1.0 else -1*np.log(1-x[0])).mean())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GDUpdate(dataRDD, W, learningRate = 0.05):\n",
    "    \"\"\"\n",
    "    Perform one gradient descent step/update.\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    Returns:\n",
    "        new_model - (array) updated coefficients, bias at index 0\n",
    "    \"\"\"\n",
    "    # add a bias 'feature' of 1 at index 0\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1])).cache()\n",
    "    \n",
    "    grad = augmentedData.map(lambda x: sum(x[0]*((1/(1+np.exp(-1*np.dot(W, x[0]))))-x[1]))).mean()\n",
    "    new_model = W - learningRate * grad \n",
    "   \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(dataRDD):\n",
    "    \"\"\"\n",
    "    Scale and center data round mean of each feature.\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (features_array, y)\n",
    "    Returns:\n",
    "        normedRDD - records are tuples of (features_array, y)\n",
    "    \"\"\"\n",
    "    featureMeans = dataRDD.map(lambda x: x[0]).mean()\n",
    "    featureStdev = np.sqrt(dataRDD.map(lambda x: x[0]).variance())\n",
    "    \n",
    "    ################ YOUR CODE HERE #############\n",
    "    normedRDD = dataRDD.map(lambda x: (((x[0] - featureMeans)/featureStdev),x[1]))\n",
    "    ################ FILL IN YOUR CODE HERE #############\n",
    "    \n",
    "    return normedRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "normedRDD = normalize(dummyRDD).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4821362365121533"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASELINE = np.array([.5,.5])\n",
    "logloss(normedRDD,BASELINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE:  Loss = 0.4821362365121533\n",
      "----------\n",
      "STEP: 1\n",
      "Loss: 0.47734994807407366\n",
      "Model: [0.516, 0.516]\n",
      "----------\n",
      "STEP: 2\n",
      "Loss: 0.47274905375004195\n",
      "Model: [0.531, 0.531]\n",
      "----------\n",
      "STEP: 3\n",
      "Loss: 0.4683239447884896\n",
      "Model: [0.546, 0.546]\n",
      "----------\n",
      "STEP: 4\n",
      "Loss: 0.4640655990000977\n",
      "Model: [0.56, 0.56]\n",
      "----------\n",
      "STEP: 5\n",
      "Loss: 0.4599655425330241\n",
      "Model: [0.575, 0.575]\n",
      "----------\n",
      "STEP: 6\n",
      "Loss: 0.4560158139732677\n",
      "Model: [0.589, 0.589]\n",
      "----------\n",
      "STEP: 7\n",
      "Loss: 0.45220893068932705\n",
      "Model: [0.603, 0.603]\n",
      "----------\n",
      "STEP: 8\n",
      "Loss: 0.4485378573286763\n",
      "Model: [0.616, 0.616]\n",
      "----------\n",
      "STEP: 9\n",
      "Loss: 0.4449959763659943\n",
      "Model: [0.63, 0.63]\n",
      "----------\n",
      "STEP: 10\n",
      "Loss: 0.4415770605987077\n",
      "Model: [0.643, 0.643]\n",
      "----------\n",
      "STEP: 11\n",
      "Loss: 0.4382752474835591\n",
      "Model: [0.656, 0.656]\n",
      "----------\n",
      "STEP: 12\n",
      "Loss: 0.43508501520803056\n",
      "Model: [0.668, 0.668]\n",
      "----------\n",
      "STEP: 13\n",
      "Loss: 0.432001160392051\n",
      "Model: [0.681, 0.681]\n",
      "----------\n",
      "STEP: 14\n",
      "Loss: 0.4290187773181404\n",
      "Model: [0.693, 0.693]\n",
      "----------\n",
      "STEP: 15\n",
      "Loss: 0.4261332385916695\n",
      "Model: [0.705, 0.705]\n",
      "----------\n",
      "STEP: 16\n",
      "Loss: 0.4233401771369949\n",
      "Model: [0.717, 0.717]\n",
      "----------\n",
      "STEP: 17\n",
      "Loss: 0.42063546943968666\n",
      "Model: [0.729, 0.729]\n",
      "----------\n",
      "STEP: 18\n",
      "Loss: 0.41801521994971963\n",
      "Model: [0.74, 0.74]\n",
      "----------\n",
      "STEP: 19\n",
      "Loss: 0.4154757465652514\n",
      "Model: [0.752, 0.752]\n",
      "----------\n",
      "STEP: 20\n",
      "Loss: 0.41301356712135673\n",
      "Model: [0.763, 0.763]\n",
      "----------\n",
      "STEP: 21\n",
      "Loss: 0.4106253868127603\n",
      "Model: [0.774, 0.774]\n",
      "----------\n",
      "STEP: 22\n",
      "Loss: 0.40830808648416084\n",
      "Model: [0.784, 0.784]\n",
      "----------\n",
      "STEP: 23\n",
      "Loss: 0.4060587117261207\n",
      "Model: [0.795, 0.795]\n",
      "----------\n",
      "STEP: 24\n",
      "Loss: 0.4038744627187028\n",
      "Model: [0.806, 0.806]\n",
      "----------\n",
      "STEP: 25\n",
      "Loss: 0.4017526847690261\n",
      "Model: [0.816, 0.816]\n",
      "----------\n",
      "STEP: 26\n",
      "Loss: 0.3996908594926945\n",
      "Model: [0.826, 0.826]\n",
      "----------\n",
      "STEP: 27\n",
      "Loss: 0.39768659659262434\n",
      "Model: [0.836, 0.836]\n",
      "----------\n",
      "STEP: 28\n",
      "Loss: 0.3957376261921391\n",
      "Model: [0.846, 0.846]\n",
      "----------\n",
      "STEP: 29\n",
      "Loss: 0.3938417916823437\n",
      "Model: [0.856, 0.856]\n",
      "----------\n",
      "STEP: 30\n",
      "Loss: 0.3919970430467178\n",
      "Model: [0.866, 0.866]\n",
      "----------\n",
      "STEP: 31\n",
      "Loss: 0.3902014306286034\n",
      "Model: [0.875, 0.875]\n",
      "----------\n",
      "STEP: 32\n",
      "Loss: 0.38845309930980276\n",
      "Model: [0.884, 0.884]\n",
      "----------\n",
      "STEP: 33\n",
      "Loss: 0.3867502830708658\n",
      "Model: [0.894, 0.894]\n",
      "----------\n",
      "STEP: 34\n",
      "Loss: 0.38509129990584207\n",
      "Model: [0.903, 0.903]\n",
      "----------\n",
      "STEP: 35\n",
      "Loss: 0.38347454706629863\n",
      "Model: [0.912, 0.912]\n",
      "----------\n",
      "STEP: 36\n",
      "Loss: 0.38189849661129216\n",
      "Model: [0.921, 0.921]\n",
      "----------\n",
      "STEP: 37\n",
      "Loss: 0.3803616912417242\n",
      "Model: [0.93, 0.93]\n",
      "----------\n",
      "STEP: 38\n",
      "Loss: 0.3788627403991157\n",
      "Model: [0.938, 0.938]\n",
      "----------\n",
      "STEP: 39\n",
      "Loss: 0.3774003166103311\n",
      "Model: [0.947, 0.947]\n",
      "----------\n",
      "STEP: 40\n",
      "Loss: 0.37597315206115167\n",
      "Model: [0.955, 0.955]\n",
      "----------\n",
      "STEP: 41\n",
      "Loss: 0.3745800353828716\n",
      "Model: [0.964, 0.964]\n",
      "----------\n",
      "STEP: 42\n",
      "Loss: 0.37321980863726295\n",
      "Model: [0.972, 0.972]\n",
      "----------\n",
      "STEP: 43\n",
      "Loss: 0.3718913644863372\n",
      "Model: [0.98, 0.98]\n",
      "----------\n",
      "STEP: 44\n",
      "Loss: 0.37059364353432983\n",
      "Model: [0.988, 0.988]\n",
      "----------\n",
      "STEP: 45\n",
      "Loss: 0.36932563183026146\n",
      "Model: [0.996, 0.996]\n",
      "----------\n",
      "STEP: 46\n",
      "Loss: 0.36808635852027466\n",
      "Model: [1.004, 1.004]\n",
      "----------\n",
      "STEP: 47\n",
      "Loss: 0.366874893639738\n",
      "Model: [1.012, 1.012]\n",
      "----------\n",
      "STEP: 48\n",
      "Loss: 0.36569034603582984\n",
      "Model: [1.02, 1.02]\n",
      "----------\n",
      "STEP: 49\n",
      "Loss: 0.3645318614119876\n",
      "Model: [1.027, 1.027]\n",
      "----------\n",
      "STEP: 50\n",
      "Loss: 0.3633986204862274\n",
      "Model: [1.035, 1.035]\n",
      "----------\n",
      "STEP: 51\n",
      "Loss: 0.3622898372559083\n",
      "Model: [1.042, 1.042]\n",
      "----------\n",
      "STEP: 52\n",
      "Loss: 0.3612047573620473\n",
      "Model: [1.05, 1.05]\n",
      "----------\n",
      "STEP: 53\n",
      "Loss: 0.3601426565467756\n",
      "Model: [1.057, 1.057]\n",
      "----------\n",
      "STEP: 54\n",
      "Loss: 0.3591028391979816\n",
      "Model: [1.064, 1.064]\n",
      "----------\n",
      "STEP: 55\n",
      "Loss: 0.3580846369756007\n",
      "Model: [1.071, 1.071]\n",
      "----------\n",
      "STEP: 56\n",
      "Loss: 0.3570874075143992\n",
      "Model: [1.078, 1.078]\n",
      "----------\n",
      "STEP: 57\n",
      "Loss: 0.35611053319845687\n",
      "Model: [1.085, 1.085]\n",
      "----------\n",
      "STEP: 58\n",
      "Loss: 0.3551534200028813\n",
      "Model: [1.092, 1.092]\n",
      "----------\n",
      "STEP: 59\n",
      "Loss: 0.35421549639859734\n",
      "Model: [1.099, 1.099]\n",
      "----------\n",
      "STEP: 60\n",
      "Loss: 0.35329621231633224\n",
      "Model: [1.106, 1.106]\n",
      "----------\n",
      "STEP: 61\n",
      "Loss: 0.35239503816618645\n",
      "Model: [1.113, 1.113]\n",
      "----------\n",
      "STEP: 62\n",
      "Loss: 0.35151146390941657\n",
      "Model: [1.119, 1.119]\n",
      "----------\n",
      "STEP: 63\n",
      "Loss: 0.3506449981792881\n",
      "Model: [1.126, 1.126]\n",
      "----------\n",
      "STEP: 64\n",
      "Loss: 0.349795167448063\n",
      "Model: [1.132, 1.132]\n",
      "----------\n",
      "STEP: 65\n",
      "Loss: 0.3489615152373781\n",
      "Model: [1.139, 1.139]\n",
      "----------\n",
      "STEP: 66\n",
      "Loss: 0.3481436013694572\n",
      "Model: [1.145, 1.145]\n",
      "----------\n",
      "STEP: 67\n",
      "Loss: 0.3473410012567602\n",
      "Model: [1.152, 1.152]\n",
      "----------\n",
      "STEP: 68\n",
      "Loss: 0.3465533052278326\n",
      "Model: [1.158, 1.158]\n",
      "----------\n",
      "STEP: 69\n",
      "Loss: 0.3457801178872618\n",
      "Model: [1.164, 1.164]\n",
      "----------\n",
      "STEP: 70\n",
      "Loss: 0.34502105750778095\n",
      "Model: [1.17, 1.17]\n",
      "----------\n",
      "STEP: 71\n",
      "Loss: 0.3442757554526848\n",
      "Model: [1.177, 1.177]\n",
      "----------\n",
      "STEP: 72\n",
      "Loss: 0.343543855626842\n",
      "Model: [1.183, 1.183]\n",
      "----------\n",
      "STEP: 73\n",
      "Loss: 0.3428250139546904\n",
      "Model: [1.189, 1.189]\n",
      "----------\n",
      "STEP: 74\n",
      "Loss: 0.3421188978837111\n",
      "Model: [1.195, 1.195]\n",
      "----------\n",
      "STEP: 75\n",
      "Loss: 0.3414251859119608\n",
      "Model: [1.2, 1.2]\n",
      "----------\n",
      "STEP: 76\n",
      "Loss: 0.3407435671383389\n",
      "Model: [1.206, 1.206]\n",
      "----------\n",
      "STEP: 77\n",
      "Loss: 0.3400737408343417\n",
      "Model: [1.212, 1.212]\n",
      "----------\n",
      "STEP: 78\n",
      "Loss: 0.3394154160361332\n",
      "Model: [1.218, 1.218]\n",
      "----------\n",
      "STEP: 79\n",
      "Loss: 0.33876831115583456\n",
      "Model: [1.224, 1.224]\n",
      "----------\n",
      "STEP: 80\n",
      "Loss: 0.3381321536109992\n",
      "Model: [1.229, 1.229]\n",
      "----------\n",
      "STEP: 81\n",
      "Loss: 0.3375066794713025\n",
      "Model: [1.235, 1.235]\n",
      "----------\n",
      "STEP: 82\n",
      "Loss: 0.3368916331215337\n",
      "Model: [1.24, 1.24]\n",
      "----------\n",
      "STEP: 83\n",
      "Loss: 0.33628676694003073\n",
      "Model: [1.246, 1.246]\n",
      "----------\n",
      "STEP: 84\n",
      "Loss: 0.3356918409917499\n",
      "Model: [1.251, 1.251]\n",
      "----------\n",
      "STEP: 85\n",
      "Loss: 0.3351066227352092\n",
      "Model: [1.257, 1.257]\n",
      "----------\n",
      "STEP: 86\n",
      "Loss: 0.3345308867425883\n",
      "Model: [1.262, 1.262]\n",
      "----------\n",
      "STEP: 87\n",
      "Loss: 0.3339644144323113\n",
      "Model: [1.267, 1.267]\n",
      "----------\n",
      "STEP: 88\n",
      "Loss: 0.3334069938134725\n",
      "Model: [1.273, 1.273]\n",
      "----------\n",
      "STEP: 89\n",
      "Loss: 0.3328584192415089\n",
      "Model: [1.278, 1.278]\n",
      "----------\n",
      "STEP: 90\n",
      "Loss: 0.33231849118454987\n",
      "Model: [1.283, 1.283]\n",
      "----------\n",
      "STEP: 91\n",
      "Loss: 0.33178701599991167\n",
      "Model: [1.288, 1.288]\n",
      "----------\n",
      "STEP: 92\n",
      "Loss: 0.3312638057202314\n",
      "Model: [1.294, 1.294]\n",
      "----------\n",
      "STEP: 93\n",
      "Loss: 0.3307486778487653\n",
      "Model: [1.299, 1.299]\n",
      "----------\n",
      "STEP: 94\n",
      "Loss: 0.33024145516340014\n",
      "Model: [1.304, 1.304]\n",
      "----------\n",
      "STEP: 95\n",
      "Loss: 0.32974196552895435\n",
      "Model: [1.309, 1.309]\n",
      "----------\n",
      "STEP: 96\n",
      "Loss: 0.32925004171736416\n",
      "Model: [1.314, 1.314]\n",
      "----------\n",
      "STEP: 97\n",
      "Loss: 0.3287655212353785\n",
      "Model: [1.319, 1.319]\n",
      "----------\n",
      "STEP: 98\n",
      "Loss: 0.3282882461594003\n",
      "Model: [1.323, 1.323]\n",
      "----------\n",
      "STEP: 99\n",
      "Loss: 0.3278180629771359\n",
      "Model: [1.328, 1.328]\n",
      "----------\n",
      "STEP: 100\n",
      "Loss: 0.3273548224357276\n",
      "Model: [1.333, 1.333]\n"
     ]
    }
   ],
   "source": [
    "#iterate updating models\n",
    "nSteps = 100\n",
    "model = BASELINE\n",
    "print(f\"BASELINE:  Loss = {logloss(normedRDD,model)}\")\n",
    "for idx in range(nSteps):\n",
    "    print(\"----------\")\n",
    "    print(f\"STEP: {idx+1}\")\n",
    "    model = GDUpdate(normedRDD, model)\n",
    "    loss = logloss(normedRDD, model)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    print(f\"Model: {[round(w,3) for w in model]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
