{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "# ** WELL HAVE TO CHANGE THIS FOR THE CLUSTER\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"hw3_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "TOYDATAFILE = './dac/trainpt1percentsample.txt'\n",
    "FIELDS = ['Label'] + ['I'+str(i) for i in range(1,14)] + ['C'+str(i) for i in range(1,27)]\n",
    "toyRDD = sc.textFile(TOYDATAFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertNumber(idx, num):\n",
    "    \"\"\"\n",
    "    convert hashes to ints\n",
    "    \"\"\"\n",
    "    if num != '':\n",
    "        if idx > 13:\n",
    "            return int(num, 16)\n",
    "        else:\n",
    "            return int(num)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def parse(line):\n",
    "    \"\"\"\n",
    "    Map tab separated record to tuple of (39 features, label)\n",
    "    \"\"\"\n",
    "    parsed = np.array([ConvertNumber(idx, num) for idx,num in enumerate(line.split('\\t'))])\n",
    "    features, label = parsed[1:], parsed[0]\n",
    "    return (features, label)\n",
    "\n",
    "def logloss(dataRDD, W):\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1]))\n",
    "    loss = (augmentedData.\n",
    "            map(lambda x: (np.clip(1 / (1 + np.exp(-1*W @ x[0])), 1e-15, 1.0 - 1e-15), x[1])).\n",
    "            map(lambda x: -1*np.log(x[0]) if x[1]==1.0 else -1*np.log(1-x[0])).mean())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Formulation\n",
    "\n",
    "## Why Predict Click Through Rate (CTR)?\n",
    "Display advertising is a multi-billion dollar industry. When a given company is deciding whether or not to buy ad space, they want to know how likely people are to click on those ads (prediction problem) and the advertiser like Criteo wants to have these predicts to back up their prices when selling ad space. Companies are also interested in what things they can change in order to get a higher click through rate. e.g., Should they use more personalization? Should they use brighter colors. A machine learning approach can help both better predictions and causal understanding.\n",
    "\n",
    "## The Data\n",
    "\n",
    "The data are a downsampled week's worth of ad-displays from CriteoLabs noting whether or not they were actually clicked. The downsampling makes it so we have a more balanced dataset with fewer non-click impressions.\n",
    "\n",
    "The dataset is large\n",
    "  - Training: 11GB\n",
    "  - Test: 1.5GB\n",
    "  \n",
    "There are 40 total tab separated fields\n",
    "  - The label (1 if clicked, 0 if not).\n",
    "  - 13 Integer fields\n",
    "  - 26 Categorical fields (hashed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\t1\t5\t0\t1382\t4\t15\t2\t181\t1\t2\t\t2\t68fd1e64\t80e26c9b\tfb936136\t7b4723c4\t25c83c98\t7e0ccccf\tde7995b8\t1f89b562\ta73ee510\ta8cd5504\tb2cb9c98\t37c9c164\t2824a5f6\t1adce6ef\t8ba8b39a\t891b62e7\te5ba7672\tf54016b9\t21ddcdc9\tb1252a9d\t07b5194c\t\t3a171ecb\tc5c50484\te8b83407\t9727dd16\n"
     ]
    }
   ],
   "source": [
    "# Example rows:\n",
    "!head -n 1 ./dac/train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though we are not told exactly what these fields correspond to, they belong to the following categories:\n",
    "  - Publisher features, such as the domain of the url where the ad was displayed;\n",
    "  - Advertiser features (advertiser id, type of products,â€¦)\n",
    "  - User features, for instance browser type;\n",
    "  - Interaction of the user with the advertiser, such as the number of the times the user visited the advertiser website.\n",
    "\n",
    "If we had a model that told us which features were important, it would still be helpful to Criteo because they would be able to tell which of the above features they should focus on moving forward. For example, if browser type of Internet Explorer showed a much lower CTR than others, Criteo might explore that type and see there is a bug where ads are not displayed well.\n",
    "\n",
    "## Question (key goal)\n",
    "\n",
    "Our question is two fold\n",
    "1. What is our best guess of the CTR for a given set of features?\n",
    "2. Which features are most important in determining the CTR?\n",
    "\n",
    "We will use logloss, recall, and precision on a validation set as our standard for 1. and use logistic regression in order to have coefficients that will help us answer 2. \n",
    "\n",
    "Logloss is good metric because it penalizes classifiers that are very confident in incorrect predictions. Thus, when we are analyzing our results that have optimized logloss, we are more confident that we have a well-calibrated model (that as our predictions increase, so do the true probabilities). Recall will give us sense of how many of the clicks we actually capture, while precision will tell us how conservative our model is with predicting clicks. \n",
    "\n",
    "Improving prediction accuracy on a validation set from a baseline guess-the-mean strategy by a small amount, even ~0.5%, has significant consequences given the huge dollar base of this billion-dollar industry. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Explanation\n",
    "\n",
    "An appropriate model to answer both of our key questions above is Logistic Regression. It is an intretatble binary classifer that often works well when we have a long feature vector with many one-hot encoded variables (e.g., it is often the go to for NLP basedline models with bag-of-words one hot encoding https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e). \n",
    "\n",
    "## 1.  What is our best guess of the CTR for a given set of features?\n",
    "\n",
    "Logistic regression works similarly to linear regression, but instead of fitting a hyper-plane to a set of training points in the feature space, it fits a sigmoid function to a set of binary-class training examples. The value of the sigmoid function at a given point in the feature space is the predicted probability that a person has clicked on the ad in our analysis.\n",
    "\n",
    "This is an improvement over linear regression for classification because its predictions are restricted to be between 0 and 1 like probabilities should be.\n",
    "\n",
    "More formally, logistic regression models the log-odds of a click as a linear function of the features:\n",
    "\n",
    "$$\n",
    "\\ln\\frac{p}{1-p} = \\beta X + e\n",
    "$$\n",
    "\n",
    "Where $p$ is a vector of the probabilities that an ad is clicked, $\\beta$ is a vector of coefficients to be estimated that define the sigmoid function, $X$ is a matrix of features in our model, and $e$ is an error term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://www.saedsayad.com/images/LogReg_1.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://www.saedsayad.com/images/LogReg_1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logisitic regression does assume that the data are linearly separable into the binary classes in the feature space, but this simple assumption is what gives us the great interpretability through coefficients. Even though this assumption doesn't perfectly hold (which means we will have some bias in our estimates), the simplicity of a linear model reduces the variance of our predicts as well. Also, given that we have a fairly large number of features, this linearity assumption may not cause too much bias in practice. Another assumption in logisitc regression is that all the points are indpendently generated. This is likely false again because some data points might come from the same customer coming back to the site and seeing the same add a second time. Even though all the feature (besides time) might be the same here, there is likely correlation between whether or not that customer will click. However, this assumption likely just increase the standard errors of our coefficients instead of biasing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimator (MLE)\n",
    "\n",
    "To estimate $\\beta$, we use the MLE. Logisitic regression assumes each label is independently drawn from a Bernoulli distribution with a probability that is a function of the features: $y_i \\sim Bernoulli(p(x_i))$. This translates to a likelihood across all $N$ datapoints of:\n",
    "\n",
    "$$\n",
    "L(\\beta) = \\Pi_{i=1}^{N} p(x_i)^{y_i}(1-p(x_i))^{1-y_i}\n",
    "$$\n",
    "\n",
    "Sums behave better than products, so we can use the monotonic log transform to make this likelihood better behaved. The log-likelihood is:\n",
    "\n",
    "$$\n",
    "l(\\beta) = \\sum_{i=1}^{N} y_i\\ln p(x_i) + (1-y_i)\\ln (1-p(x_i)) \\\\\n",
    "= \\sum_{i=1}^{N} y_i\\ln \\frac{p(x_i)}{1-p(x_i)} + \\ln (1-p(x_i))\n",
    "$$\n",
    "\n",
    "Then we can substitute $\\beta x_i$ in for the log odds and $\\frac{1}{1+e^{-\\beta x_i}}$ for $p$:\n",
    "\n",
    "$$\n",
    "= \\sum_{i=1}^{N} y_i \\beta x_i + \\ln (1-\\frac{1}{1+e^{-\\beta x_i}}) \\\\\n",
    "= \\sum_{i=1}^{N} y_i \\beta x_i + \\ln (\\frac{e^{-\\beta x_i}}{1+e^{-\\beta x_i}}) \\\\\n",
    "= \\sum_{i=1}^{N} y_i \\beta x_i - \\beta x_i - \\ln( 1+e^{-\\beta x_i}) \n",
    "$$\n",
    "\n",
    "This function has no closed-form maximum, so we use gradient ascent to climb the surface of $l$ in the space of $\\beta$. Differentiating the above with respect to $\\beta$ gives us the gradient.\n",
    "\n",
    "$$\n",
    "\\nabla_{\\beta} l = \\sum_{i=1}^{N} x_i y_i - x_i + \\frac{x_i e^{-\\beta x_i}}{1+e^{-\\beta x_i}} \\\\\n",
    "= x_i (y_i - \\frac{1}{1+e^{-\\beta x_i}})\n",
    "$$\n",
    "\n",
    "Then, our algorithm is as follows:\n",
    "\n",
    "1. Specify a baseline $\\hat{\\beta}$ as an initialization\n",
    "2. Calculate $\\frac{\\nabla_{\\beta} l}{N}$. Note here that each $i$'s contribution to the sum is independent, so we can distribute the work across a cluster to calculate the gradient in each iteration. We also change to the average contribution per $i$ by dividing by $N$ instead of using the sum. This way, the magnitude is not proportional to the number of data points and we can keep the same learning rate for any number of training points.\n",
    "3. Update $\\hat{\\beta} := \\hat{\\beta} + \\alpha \\frac{\\nabla_{\\beta} l}{N}$. Here $\\alpha$ is the learning rate.\n",
    "4. Repeat 2-3 for the specified number of iterations. (this could be improved to stop when the calculated gradient is small enough, but to keep things simple we have a fixed number of iterations).\n",
    "\n",
    "\n",
    "Luckily the log likelihood $l$ is a concave function, so we know that if we set our learning rate small enough, we will reach a global maximum. (see https://homes.cs.washington.edu/~marcotcr/blog/concavity/)\n",
    "\n",
    "This will be accomplished by the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GDUpdate(dataRDD, W, learningRate = 0.05, regType = None, regParam = 0):\n",
    "    \"\"\"\n",
    "    Perform one gradient descent step/update.\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    Returns:\n",
    "        new_model - (array) updated coefficients, bias at index 0\n",
    "    \"\"\"\n",
    "    # add a bias 'feature' of 1 at index 0\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1])).cache()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # use negative log likelihood to keep it descent\n",
    "    grad = augmentedData.map(lambda x: -x[0]*(x[1] - (1/(1+np.exp(-1*np.dot(W, x[0])))))).mean()\n",
    "    if regType=='l1':\n",
    "        grad[1:] += 2 * regParam * W[1:]\n",
    "    elif regType=='l2':\n",
    "        grad[1:] += regParam * np.sign(W[1:])\n",
    "\n",
    "    new_model = W - learningRate * grad \n",
    "   \n",
    "    return new_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like linear regression, logistic regression can overfit to the training data if too many features are used. To adjust for this, we can use L1 or L2 regularization to select coefficients / keep coefficients small. To do so we can add a penalty to the log-likelihood to get a new function $J$ to climb:\n",
    "\n",
    "$$\n",
    "J = l(\\beta) - \\lambda penalty(\\beta)\n",
    "$$\n",
    "\n",
    "L2 Regularization would be \n",
    "\n",
    "$$\n",
    "J = l(\\beta) - \\lambda \\sum_{j=1}^{m} \\beta_j^2 \n",
    "$$\n",
    "\n",
    "with new corresponding gradient of:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\beta} J = x_i (y_i - \\frac{1}{1+e^{-\\beta x_i}}) - 2 \\lambda \\beta_j\n",
    "$$\n",
    "\n",
    "L1 would be \n",
    "\n",
    "$$\n",
    "J = l(\\beta) - \\lambda \\sum_{j=1}^{m} |\\beta_j| \n",
    "$$\n",
    "\n",
    "with new corresponding gradient of:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\beta} J = x_i (y_i - \\frac{1}{1+e^{-\\beta x_i}}) - \\lambda sign(\\beta_j)\n",
    "$$\n",
    "\n",
    "Our algorithm could use the adjusted $\\nabla_{\\beta} J$ instead of $\\nabla_{\\beta} l$ in steps 2 and 3 to avoid over-fitting or to choose which variables are important.\n",
    "\n",
    "Here $\\lambda$, the regularization parameter, controlls the level of regularization. The higher the value of $\\lambda$, the more bias we will add, but the less variance we will have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "Once we have $\\hat{\\beta}$, to make predicted probabilities we solve for $p$ in the log odds formula above:\n",
    "$$\n",
    "\\hat{p} = \\frac{1}{1+e^{-\\hat{\\beta} x_i}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, our log loss metric is:\n",
    "$$\n",
    "logloss = \\frac{1}{N} \\sum_{i=1}^{N} -y_i \\log \\hat{p_i} - (1-y_i) \\log(1-\\hat{p_i})\n",
    "$$\n",
    "\n",
    "Again each $i$'s contribution to the logloss is independent so the calculation on a validation set can be distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logloss(dataRDD, W):\n",
    "    \"\"\"\n",
    "    Calculates the mean log loss from an RDD of (np.array features, labels)\n",
    "    and a weights array W\n",
    "    \"\"\"\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1]))\n",
    "    loss = (augmentedData.\n",
    "            map(lambda x: (np.clip(1 / (1 + np.exp(-1*W @ x[0])), 1e-15, 1.0 - 1e-15), x[1])).\n",
    "            map(lambda x: -1*np.log(x[0]) if x[1]==1.0 else -1*np.log(1-x[0])).mean())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Which features are most important in determining the CTR?\n",
    "\n",
    "The estimated coefficients $\\hat{\\beta}$ also enable us to interpret the impact of a given feature. For the numeric features, the interpretation is that if the value of featuer $x_j$ increased by one (or if the standard deviation increased by one if normalized to unit variance), the log odds would be predicted to increase by $\\hat{\\beta_j}$. For features that are one-hot encoded (like our categorical features), $\\beta_j$ represents the increase in the log odds from going from the base category (the one category left out of the regression) to the category represented by a one in $x_j$. \n",
    "\n",
    "If we standardize all features, the ones with the highest magnitude coefficients could be considered to have the most impact of the probability of a click. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Example without regularization\n",
    "\n",
    "To illustrate logistic regression, we use a 0.1% percent random sample from the training data created with `!shuf -n 45841 train.txt > trainpt1percentsample.txt`. We'll train a model:\n",
    "\n",
    "$$\n",
    "logoddsclick = \\beta_0 + \\beta_1 I_1 + \\beta_2 C_2\n",
    "$$\n",
    "\n",
    "Here $I$ is a numeric field from the data and $C$ is a categorical transformed into a binary flag (like a one hot encoding for a 2-valued categorical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first get the data ready\n",
    "toyRDD_intermediate = (\n",
    "    toyRDD.map(parse) # parse the raw data\n",
    "    # pull one numeric and one categorical feature transformed into a binary indicator (like a one hot encoding)\n",
    "    .map(lambda x: (np.array([x[0][1], 0 if x[0][26] < 1 else 1]), x[1]))\n",
    ") \n",
    "# pull mean and standard deviation to standardize numeric feature later \n",
    "mean_numeric =  toyRDD_intermediate.map(lambda x: x[0][0]).mean()\n",
    "std_numeric =  toyRDD_intermediate.map(lambda x: x[0][0]).variance()**0.5   \n",
    "toytrainRDD, toyheldOutRDD =(\n",
    "    # standardize the numeric feature to stablize the algorithm\n",
    "    toyRDD_intermediate.map(lambda x: (np.array([(x[0][0] - mean_numeric)/std_numeric,x[0][1]]), x[1]))\n",
    "    # separate into train and validation sets\n",
    "    .randomSplit([0.8,0.2], seed = 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Model: [-1.0, -1.5, 0.1]\n",
      "Initial Train Loss = 0.673093259449749\n",
      "Initial Held-Out Loss = 0.6774833986160038\n",
      "-----------------------------------------------------\n",
      "iteration # 0\n",
      "New Model: [-1.082, -1.411, 0.018]\n",
      "Train Loss = 0.6554675536953056\n",
      "Held-Out Loss = 0.6595849855422558\n",
      "-----------------------------------------------------\n",
      "iteration # 1\n",
      "New Model: [-1.126, -1.33, -0.026]\n",
      "Train Loss = 0.6463127417271598\n",
      "Held-Out Loss = 0.6501576256516545\n",
      "-----------------------------------------------------\n",
      "iteration # 2\n",
      "New Model: [-1.149, -1.254, -0.049]\n",
      "Train Loss = 0.6398856840016004\n",
      "Held-Out Loss = 0.6435087839347325\n",
      "-----------------------------------------------------\n",
      "iteration # 3\n",
      "New Model: [-1.16, -1.181, -0.06]\n",
      "Train Loss = 0.6344722234885993\n",
      "Held-Out Loss = 0.6378745296155612\n",
      "-----------------------------------------------------\n",
      "iteration # 4\n",
      "New Model: [-1.165, -1.109, -0.065]\n",
      "Train Loss = 0.6294770195631495\n",
      "Held-Out Loss = 0.6326607903242772\n",
      "-----------------------------------------------------\n",
      "iteration # 5\n",
      "New Model: [-1.165, -1.039, -0.065]\n",
      "Train Loss = 0.6246903346014315\n",
      "Held-Out Loss = 0.6276588104668985\n",
      "-----------------------------------------------------\n",
      "iteration # 6\n",
      "New Model: [-1.163, -0.971, -0.063]\n",
      "Train Loss = 0.6200386224088442\n",
      "Held-Out Loss = 0.622795857104668\n",
      "-----------------------------------------------------\n",
      "iteration # 7\n",
      "New Model: [-1.159, -0.903, -0.059]\n",
      "Train Loss = 0.6154577584567439\n",
      "Held-Out Loss = 0.61804969729535\n",
      "-----------------------------------------------------\n",
      "iteration # 8\n",
      "New Model: [-1.154, -0.836, -0.054]\n",
      "Train Loss = 0.610962568821912\n",
      "Held-Out Loss = 0.6134178194861278\n",
      "-----------------------------------------------------\n",
      "iteration # 9\n",
      "New Model: [-1.149, -0.77, -0.049]\n",
      "Train Loss = 0.6065824855727087\n",
      "Held-Out Loss = 0.6089060976978429\n",
      "-----------------------------------------------------\n",
      "iteration # 10\n",
      "New Model: [-1.144, -0.705, -0.044]\n",
      "Train Loss = 0.6023272423617381\n",
      "Held-Out Loss = 0.604524651637536\n",
      "-----------------------------------------------------\n",
      "iteration # 11\n",
      "New Model: [-1.138, -0.641, -0.038]\n",
      "Train Loss = 0.5982095783928255\n",
      "Held-Out Loss = 0.600286450065883\n",
      "-----------------------------------------------------\n",
      "iteration # 12\n",
      "New Model: [-1.133, -0.578, -0.033]\n",
      "Train Loss = 0.5942449624280153\n",
      "Held-Out Loss = 0.5962069811605728\n",
      "-----------------------------------------------------\n",
      "iteration # 13\n",
      "New Model: [-1.127, -0.516, -0.027]\n",
      "Train Loss = 0.5904517436878772\n",
      "Held-Out Loss = 0.592304327460379\n",
      "-----------------------------------------------------\n",
      "iteration # 14\n",
      "New Model: [-1.122, -0.456, -0.022]\n",
      "Train Loss = 0.5868514179763182\n",
      "Held-Out Loss = 0.5885993412075089\n",
      "-----------------------------------------------------\n",
      "iteration # 15\n",
      "New Model: [-1.117, -0.398, -0.017]\n",
      "Train Loss = 0.5834687816208454\n",
      "Held-Out Loss = 0.5851157110003848\n",
      "-----------------------------------------------------\n",
      "iteration # 16\n",
      "New Model: [-1.113, -0.342, -0.013]\n",
      "Train Loss = 0.5803317052486374\n",
      "Held-Out Loss = 0.5818796869625096\n",
      "-----------------------------------------------------\n",
      "iteration # 17\n",
      "New Model: [-1.109, -0.288, -0.009]\n",
      "Train Loss = 0.5774701648336196\n",
      "Held-Out Loss = 0.5789191560431535\n",
      "-----------------------------------------------------\n",
      "iteration # 18\n",
      "New Model: [-1.105, -0.237, -0.005]\n",
      "Train Loss = 0.5749141013201075\n",
      "Held-Out Loss = 0.5762617009514631\n",
      "-----------------------------------------------------\n",
      "iteration # 19\n",
      "New Model: [-1.101, -0.189, -0.001]\n",
      "Train Loss = 0.5726897845046274\n",
      "Held-Out Loss = 0.5739313538295665\n"
     ]
    }
   ],
   "source": [
    "# train the logistic regression\n",
    "W = np.array([-1,-1.5,.1])\n",
    "print(f\"Initial Model: {[round(w,3) for w in W]}\")\n",
    "print(f\"Initial Train Loss = {logloss(toytrainRDD,W)}\")\n",
    "print(f\"Initial Held-Out Loss = {logloss(toyheldOutRDD,W)}\")\n",
    "n_iterations = 20\n",
    "for i in range(n_iterations):\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print(\"iteration # {}\".format(i))\n",
    "    W = GDUpdate(toytrainRDD, W, learningRate = 1)\n",
    "    print(f\"New Model: {[round(w,3) for w in W]}\")\n",
    "    print(f\"Train Loss = {logloss(toytrainRDD,W)}\")\n",
    "    print(f\"Held-Out Loss = {logloss(toyheldOutRDD,W)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logloss gets lower as we climb the likelihood function.\n",
    "\n",
    "Our fit model is\n",
    "$$\n",
    "logoddsclick= -1.101 + -0.189 I_1 + -0.001 C_2\n",
    "$$\n",
    "\n",
    "So each standard deviation increase in $I$ makes the log-odds of a click predicted .189 lower and going from the category variable havign a value below 1 to above 1 hardly changes the log odds with a small .001 predicted decrease. We can visualize the predicted probability of our model for each value of $C_2$ across values of I:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "I = np.linspace(-20,20)\n",
    "P_C0 = 1/(1+np.exp(-(W[0]+W[1]*I)))\n",
    "P_C1 = 1/(1+np.exp(-(W[0]+W[1]*I+W[2])))\n",
    "\n",
    "# set up figure\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (15,5))\n",
    "\n",
    "ax1.plot(I,P_C0)\n",
    "ax1.set_title('Predicted Probability vs Value of Numeric Feature, Categorical Below 1', color='0.1')\n",
    "ax1.tick_params(axis='both', colors='0.1')\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(I,P_C0)\n",
    "ax2.set_title('Predicted Probability vs Value of Numeric Feature, Categorical >= 1', color='0.1')\n",
    "ax2.tick_params(axis='both', colors='0.1')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both curves are bounded between 0 and 1 and decrease as $I_1$ increases. They are nearly identical curves because the coefficient on $C_2$ is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though given our data above we don't seem to be overfitting because the training loss is close to the validation loss, we only have 2 features in this model. If we had many more regularization would make sense to keep coefficeint values small. For example, if we use L2 regularization instead we see the estimated coefficient values are all below 1 in magnitude:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of Course Concepts\n",
    "\n",
    "This assignment encompasses a number of the key concepts of W261. \n",
    "\n",
    "## bias variance tradeoff / model complexity / regularization  / model assumptions\n",
    "Instead of using all parameters in our model, we balanced bias and variance by removing variables that were not important according to L1 regularization. This reduces the variance and increases the bias. Also, our general choice of a simple model (logistic regression) likely adds bias due to the linearly separable assumption, but reduces variance for the same reason.\n",
    "\n",
    "## GD - convex optimization / Batch / Normalization\n",
    "We have also implemented a scalable batch gradient descent approach. The approach is scalable because it distributes the task of calculating the gradient accross a cluster. Each data point's contribution to the gradient is independent so this calculation is embarrasingly parallel. We also know that if we set the learning rate small enough and have a large enough number of iterations, we will reach the coefficients that give the global maximum likelihood. This is because the log likelihood function we are climbing is concave such that there is only 1 global maximum and no local ones that we might accidentally end up in. Wihtout standardizing the variables, gradient descent becomes unstable. Thus, we used a distributed method to make all numeric variables have 0 mean and unit variance. \n",
    "\n",
    "Given that this dataset was a small subset of the data that would actually be used in practice, other methods would not be able to handle it like our method. For example, sequentially moving through the data on one node would take too long given that we would have to loop over it multiple times. Holding all the data in memory on a single node is also unreasonably expensive given that the real data likely has many billions of rows.\n",
    "\n",
    "## Caching and Broadcasting\n",
    "\n",
    "**(we should make sure we actually do use broadcasting and caching so this is true)**\n",
    "To make our computations faster, we cache the training data set so it is availble in memory for each iteration. We also broadcast the value of the coefficients so that they are available on all nodes in doing the gradient calculation and update. Without caching, a lot more time would be spent processing the data. Without broadcasting explicitly, the master node would have to do more work. With broadcasting, the worker nodes have the information they need locally.\n",
    "\n",
    "## One Hot Encoding / Feature Selection\n",
    "Given the high-cardinality categorical features of this dataset, we have needed to do a lot of feature selection. Our EDA informed which variables might be important. From there, we also used L1 regularization to see which coefficients dropped. At the same time, the cardinality of our categorical features means that we could not one hot encode all values. However, because we do not know the difference between the different values, the hashing trick to reduce dimensionality works well. Then we can one hot encode the categorical with its reduced cardinality having a column for each of the new values except one base value. Then, the coefficients on these one-hot encoding fields are the changes in log likelihood that occur if we move from the base value of the categorical variable to the value encoded in that column.\n",
    "\n",
    "## Spark vs Map Reduce\n",
    "\n",
    "We chose to do our implementation in Spark instead of Map Reduce. Spark is much more efficient when it comes to multi-pass computations like our iterative gradient calculations. Spark allows us to do our data transformations without requiring us to write to disk after each step. This heavy IO in the map reduce framework would make our data preparation phase much longer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this project we have used data from Criteo Labs to train a logisitc regression that 1. predicts click through rates given available features and 2. gives us a high level understand of which features are more important in determining the predicted probability of a click. We conducted a thourough EDA indicating that many categorical variables had high cardinality and needed to be reduced to a smaller space before one hot encoding. Our algorithm's gradient descent implementation in Spark is scalable. We found a ** improvement over the baseline guess-the-mean strategy. Overall our methods illustrate a number of the key concepts covered in the course, which allow us to have a solution that scales to the true size of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
